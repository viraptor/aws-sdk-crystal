module AwsSdk
  module CloudWatchLogs
    class Client
      getter endpoint : String
      getter endpoint_headers : Hash(String, String)
      getter region : String
      getter credentials : AwsSdk::Runtime::Credentials
      getter transport : AwsSdk::Runtime::Transport
      getter runtime : AwsSdk::Runtime::Client

      def initialize(
        endpoint : String? = nil,
        region : String? = nil,
        credentials : AwsSdk::Runtime::Credentials? = nil,
        profile : String? = nil,
        use_fips : Bool? = nil,
        use_dualstack : Bool? = nil,
        transport : AwsSdk::Runtime::Transport = AwsSdk::Runtime::HttpTransport.new
      )
        @transport = transport
        @region = region || AwsSdk::Runtime::Defaults.resolve_region(profile, @transport)
        @credentials = credentials || AwsSdk::Runtime::Defaults.resolve_credentials(profile, @transport)
        endpoint_provider = AwsSdk::Runtime::EndpointProvider.new(Model::ENDPOINT_PREFIX, Model::ENDPOINT_RULE_SET_JSON)
        endpoint_result = endpoint_provider.resolve(@region, endpoint, use_fips, use_dualstack)
        @endpoint = endpoint_result.url
        @endpoint_headers = endpoint_result.headers
        @runtime = AwsSdk::Runtime::Client.new(@endpoint, @region, Model::SIGNING_NAME, @credentials, @transport, @endpoint_headers)
      end

      # Associates the specified KMS key with either one log group in the account, or with all stored
      # CloudWatch Logs query insights results in the account. When you use AssociateKmsKey , you specify
      # either the logGroupName parameter or the resourceIdentifier parameter. You can't specify both of
      # those parameters in the same operation. Specify the logGroupName parameter to cause log events
      # ingested into that log group to be encrypted with that key. Only the log events ingested after the
      # key is associated are encrypted with that key. Associating a KMS key with a log group overrides any
      # existing associations between the log group and a KMS key. After a KMS key is associated with a log
      # group, all newly ingested data for the log group is encrypted using the KMS key. This association is
      # stored as long as the data encrypted with the KMS key is still within CloudWatch Logs. This enables
      # CloudWatch Logs to decrypt this data whenever it is requested. Associating a key with a log group
      # does not cause the results of queries of that log group to be encrypted with that key. To have query
      # results encrypted with a KMS key, you must use an AssociateKmsKey operation with the
      # resourceIdentifier parameter that specifies a query-result resource. Specify the resourceIdentifier
      # parameter with a query-result resource, to use that key to encrypt the stored results of all future
      # StartQuery operations in the account. The response from a GetQueryResults operation will still
      # return the query results in plain text. Even if you have not associated a key with your query
      # results, the query results are encrypted when stored, using the default CloudWatch Logs method. If
      # you run a query from a monitoring account that queries logs in a source account, the query results
      # key from the monitoring account, if any, is used. If you delete the key that is used to encrypt log
      # events or log group query results, then all the associated stored log events or query results that
      # were encrypted with that key will be unencryptable and unusable. CloudWatch Logs supports only
      # symmetric KMS keys. Do not associate an asymmetric KMS key with your log group or query results. For
      # more information, see Using Symmetric and Asymmetric Keys . It can take up to 5 minutes for this
      # operation to take effect. If you attempt to associate a KMS key with a log group but the KMS key
      # does not exist or the KMS key is disabled, you receive an InvalidParameterException error.
      def associate_kms_key(
        kms_key_id : String,
        log_group_name : String? = nil,
        resource_identifier : String? = nil
      ) : Nil
        input = Types::AssociateKmsKeyRequest.new(kms_key_id: kms_key_id, log_group_name: log_group_name, resource_identifier: resource_identifier)
        associate_kms_key(input)
      end
      def associate_kms_key(input : Types::AssociateKmsKeyRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::ASSOCIATE_KMS_KEY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Associates a data source with an S3 Table Integration for query access in the 'logs' namespace. This
      # enables querying log data using analytics engines that support Iceberg such as Amazon Athena, Amazon
      # Redshift, and Apache Spark.
      def associate_source_to_s3_table_integration(
        data_source : Types::DataSource,
        integration_arn : String
      ) : Types::AssociateSourceToS3TableIntegrationResponse
        input = Types::AssociateSourceToS3TableIntegrationRequest.new(data_source: data_source, integration_arn: integration_arn)
        associate_source_to_s3_table_integration(input)
      end
      def associate_source_to_s3_table_integration(input : Types::AssociateSourceToS3TableIntegrationRequest) : Types::AssociateSourceToS3TableIntegrationResponse
        request = Protocol::JsonRpc.build_request(Model::ASSOCIATE_SOURCE_TO_S3_TABLE_INTEGRATION, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::AssociateSourceToS3TableIntegrationResponse, "AssociateSourceToS3TableIntegration")
      end

      # Cancels the specified export task. The task must be in the PENDING or RUNNING state.
      def cancel_export_task(
        task_id : String
      ) : Nil
        input = Types::CancelExportTaskRequest.new(task_id: task_id)
        cancel_export_task(input)
      end
      def cancel_export_task(input : Types::CancelExportTaskRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::CANCEL_EXPORT_TASK, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Cancels an active import task and stops importing data from the CloudTrail Lake Event Data Store.
      def cancel_import_task(
        import_id : String
      ) : Types::CancelImportTaskResponse
        input = Types::CancelImportTaskRequest.new(import_id: import_id)
        cancel_import_task(input)
      end
      def cancel_import_task(input : Types::CancelImportTaskRequest) : Types::CancelImportTaskResponse
        request = Protocol::JsonRpc.build_request(Model::CANCEL_IMPORT_TASK, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::CancelImportTaskResponse, "CancelImportTask")
      end

      # Creates a delivery . A delivery is a connection between a logical delivery source and a logical
      # delivery destination that you have already created. Only some Amazon Web Services services support
      # being configured as a delivery source using this operation. These services are listed as Supported
      # [V2 Permissions] in the table at Enabling logging from Amazon Web Services services. A delivery
      # destination can represent a log group in CloudWatch Logs, an Amazon S3 bucket, a delivery stream in
      # Firehose, or X-Ray. To configure logs delivery between a supported Amazon Web Services service and a
      # destination, you must do the following: Create a delivery source, which is a logical object that
      # represents the resource that is actually sending the logs. For more information, see
      # PutDeliverySource . Create a delivery destination , which is a logical object that represents the
      # actual delivery destination. For more information, see PutDeliveryDestination . If you are
      # delivering logs cross-account, you must use PutDeliveryDestinationPolicy in the destination account
      # to assign an IAM policy to the destination. This policy allows delivery to that destination. Use
      # CreateDelivery to create a delivery by pairing exactly one delivery source and one delivery
      # destination. You can configure a single delivery source to send logs to multiple destinations by
      # creating multiple deliveries. You can also create multiple deliveries to configure multiple delivery
      # sources to send logs to the same delivery destination. To update an existing delivery configuration,
      # use UpdateDeliveryConfiguration .
      def create_delivery(
        delivery_destination_arn : String,
        delivery_source_name : String,
        field_delimiter : String? = nil,
        record_fields : Array(String)? = nil,
        s3_delivery_configuration : Types::S3DeliveryConfiguration? = nil,
        tags : Hash(String, String)? = nil
      ) : Types::CreateDeliveryResponse
        input = Types::CreateDeliveryRequest.new(delivery_destination_arn: delivery_destination_arn, delivery_source_name: delivery_source_name, field_delimiter: field_delimiter, record_fields: record_fields, s3_delivery_configuration: s3_delivery_configuration, tags: tags)
        create_delivery(input)
      end
      def create_delivery(input : Types::CreateDeliveryRequest) : Types::CreateDeliveryResponse
        request = Protocol::JsonRpc.build_request(Model::CREATE_DELIVERY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::CreateDeliveryResponse, "CreateDelivery")
      end

      # Creates an export task so that you can efficiently export data from a log group to an Amazon S3
      # bucket. When you perform a CreateExportTask operation, you must use credentials that have permission
      # to write to the S3 bucket that you specify as the destination. Exporting log data to S3 buckets that
      # are encrypted by KMS is supported. Exporting log data to Amazon S3 buckets that have S3 Object Lock
      # enabled with a retention period is also supported. Exporting to S3 buckets that are encrypted with
      # AES-256 is supported. This is an asynchronous call. If all the required information is provided,
      # this operation initiates an export task and responds with the ID of the task. After the task has
      # started, you can use DescribeExportTasks to get the status of the export task. Each account can only
      # have one active ( RUNNING or PENDING ) export task at a time. To cancel an export task, use
      # CancelExportTask . You can export logs from multiple log groups or multiple time ranges to the same
      # S3 bucket. To separate log data for each export task, specify a prefix to be used as the Amazon S3
      # key prefix for all exported objects. We recommend that you don't regularly export to Amazon S3 as a
      # way to continuously archive your logs. For that use case, we instead recommend that you use
      # subscriptions. For more information about subscriptions, see Real-time processing of log data with
      # subscriptions . Time-based sorting on chunks of log data inside an exported file is not guaranteed.
      # You can sort the exported log field data by using Linux utilities.
      def create_export_task(
        destination : String,
        from : Int64,
        log_group_name : String,
        to : Int64,
        destination_prefix : String? = nil,
        log_stream_name_prefix : String? = nil,
        task_name : String? = nil
      ) : Types::CreateExportTaskResponse
        input = Types::CreateExportTaskRequest.new(destination: destination, from: from, log_group_name: log_group_name, to: to, destination_prefix: destination_prefix, log_stream_name_prefix: log_stream_name_prefix, task_name: task_name)
        create_export_task(input)
      end
      def create_export_task(input : Types::CreateExportTaskRequest) : Types::CreateExportTaskResponse
        request = Protocol::JsonRpc.build_request(Model::CREATE_EXPORT_TASK, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::CreateExportTaskResponse, "CreateExportTask")
      end

      # Starts an import from a data source to CloudWatch Log and creates a managed log group as the
      # destination for the imported data. Currently, CloudTrail Event Data Store is the only supported data
      # source. The import task must satisfy the following constraints: The specified source must be in an
      # ACTIVE state. The API caller must have permissions to access the data in the provided source and to
      # perform iam:PassRole on the provided import role which has the same permissions, as described below.
      # The provided IAM role must trust the "cloudtrail.amazonaws.com" principal and have the following
      # permissions: cloudtrail:GetEventDataStoreData logs:CreateLogGroup logs:CreateLogStream
      # logs:PutResourcePolicy (If source has an associated AWS KMS Key) kms:Decrypt (If source has an
      # associated AWS KMS Key) kms:GenerateDataKey Example IAM policy for provided import role: [ {
      # "Effect": "Allow", "Action": "iam:PassRole", "Resource":
      # "arn:aws:iam::123456789012:role/apiCallerCredentials", "Condition": { "StringLike": {
      # "iam:AssociatedResourceARN":
      # "arn:aws:logs:us-east-1:123456789012:log-group:aws/cloudtrail/f1d45bff-d0e3-4868-b5d9-2eb678aa32fb:*"
      # } } }, { "Effect": "Allow", "Action": [ "cloudtrail:GetEventDataStoreData" ], "Resource": [
      # "arn:aws:cloudtrail:us-east-1:123456789012:eventdatastore/f1d45bff-d0e3-4868-b5d9-2eb678aa32fb" ] },
      # { "Effect": "Allow", "Action": [ "logs:CreateImportTask", "logs:CreateLogGroup",
      # "logs:CreateLogStream", "logs:PutResourcePolicy" ], "Resource": [
      # "arn:aws:logs:us-east-1:123456789012:log-group:/aws/cloudtrail/*" ] }, { "Effect": "Allow",
      # "Action": [ "kms:Decrypt", "kms:GenerateDataKey" ], "Resource": [
      # "arn:aws:kms:us-east-1:123456789012:key/12345678-1234-1234-1234-123456789012" ] } ] If the import
      # source has a customer managed key, the "cloudtrail.amazonaws.com" principal needs permissions to
      # perform kms:Decrypt and kms:GenerateDataKey. There can be no more than 3 active imports per account
      # at a given time. The startEventTime must be less than or equal to endEventTime. The data being
      # imported must be within the specified source's retention period.
      def create_import_task(
        import_role_arn : String,
        import_source_arn : String,
        import_filter : Types::ImportFilter? = nil
      ) : Types::CreateImportTaskResponse
        input = Types::CreateImportTaskRequest.new(import_role_arn: import_role_arn, import_source_arn: import_source_arn, import_filter: import_filter)
        create_import_task(input)
      end
      def create_import_task(input : Types::CreateImportTaskRequest) : Types::CreateImportTaskResponse
        request = Protocol::JsonRpc.build_request(Model::CREATE_IMPORT_TASK, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::CreateImportTaskResponse, "CreateImportTask")
      end

      # Creates an anomaly detector that regularly scans one or more log groups and look for patterns and
      # anomalies in the logs. An anomaly detector can help surface issues by automatically discovering
      # anomalies in your log event traffic. An anomaly detector uses machine learning algorithms to scan
      # log events and find patterns . A pattern is a shared text structure that recurs among your log
      # fields. Patterns provide a useful tool for analyzing large sets of logs because a large number of
      # log events can often be compressed into a few patterns. The anomaly detector uses pattern
      # recognition to find anomalies , which are unusual log events. It uses the evaluationFrequency to
      # compare current log events and patterns with trained baselines. Fields within a pattern are called
      # tokens . Fields that vary within a pattern, such as a request ID or timestamp, are referred to as
      # dynamic tokens and represented by &lt;*&gt; . The following is an example of a pattern: [INFO]
      # Request time: &lt;*&gt; ms This pattern represents log events like [INFO] Request time: 327 ms and
      # other similar log events that differ only by the number, in this csse 327. When the pattern is
      # displayed, the different numbers are replaced by &lt;*&gt; Any parts of log events that are masked
      # as sensitive data are not scanned for anomalies. For more information about masking sensitive data,
      # see Help protect sensitive log data with masking .
      def create_log_anomaly_detector(
        log_group_arn_list : Array(String),
        anomaly_visibility_time : Int64? = nil,
        detector_name : String? = nil,
        evaluation_frequency : String? = nil,
        filter_pattern : String? = nil,
        kms_key_id : String? = nil,
        tags : Hash(String, String)? = nil
      ) : Types::CreateLogAnomalyDetectorResponse
        input = Types::CreateLogAnomalyDetectorRequest.new(log_group_arn_list: log_group_arn_list, anomaly_visibility_time: anomaly_visibility_time, detector_name: detector_name, evaluation_frequency: evaluation_frequency, filter_pattern: filter_pattern, kms_key_id: kms_key_id, tags: tags)
        create_log_anomaly_detector(input)
      end
      def create_log_anomaly_detector(input : Types::CreateLogAnomalyDetectorRequest) : Types::CreateLogAnomalyDetectorResponse
        request = Protocol::JsonRpc.build_request(Model::CREATE_LOG_ANOMALY_DETECTOR, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::CreateLogAnomalyDetectorResponse, "CreateLogAnomalyDetector")
      end

      # Creates a log group with the specified name. You can create up to 1,000,000 log groups per Region
      # per account. You must use the following guidelines when naming a log group: Log group names must be
      # unique within a Region for an Amazon Web Services account. Log group names can be between 1 and 512
      # characters long. Log group names consist of the following characters: a-z, A-Z, 0-9, '_'
      # (underscore), '-' (hyphen), '/' (forward slash), '.' (period), and '#' (number sign) Log group names
      # can't start with the string aws/ When you create a log group, by default the log events in the log
      # group do not expire. To set a retention policy so that events expire and are deleted after a
      # specified time, use PutRetentionPolicy . If you associate an KMS key with the log group, ingested
      # data is encrypted using the KMS key. This association is stored as long as the data encrypted with
      # the KMS key is still within CloudWatch Logs. This enables CloudWatch Logs to decrypt this data
      # whenever it is requested. If you attempt to associate a KMS key with the log group but the KMS key
      # does not exist or the KMS key is disabled, you receive an InvalidParameterException error.
      # CloudWatch Logs supports only symmetric KMS keys. Do not associate an asymmetric KMS key with your
      # log group. For more information, see Using Symmetric and Asymmetric Keys .
      def create_log_group(
        log_group_name : String,
        deletion_protection_enabled : Bool? = nil,
        kms_key_id : String? = nil,
        log_group_class : String? = nil,
        tags : Hash(String, String)? = nil
      ) : Nil
        input = Types::CreateLogGroupRequest.new(log_group_name: log_group_name, deletion_protection_enabled: deletion_protection_enabled, kms_key_id: kms_key_id, log_group_class: log_group_class, tags: tags)
        create_log_group(input)
      end
      def create_log_group(input : Types::CreateLogGroupRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::CREATE_LOG_GROUP, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Creates a log stream for the specified log group. A log stream is a sequence of log events that
      # originate from a single source, such as an application instance or a resource that is being
      # monitored. There is no limit on the number of log streams that you can create for a log group. There
      # is a limit of 50 TPS on CreateLogStream operations, after which transactions are throttled. You must
      # use the following guidelines when naming a log stream: Log stream names must be unique within the
      # log group. Log stream names can be between 1 and 512 characters long. Don't use ':' (colon) or '*'
      # (asterisk) characters.
      def create_log_stream(
        log_group_name : String,
        log_stream_name : String
      ) : Nil
        input = Types::CreateLogStreamRequest.new(log_group_name: log_group_name, log_stream_name: log_stream_name)
        create_log_stream(input)
      end
      def create_log_stream(input : Types::CreateLogStreamRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::CREATE_LOG_STREAM, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Creates a scheduled query that runs CloudWatch Logs Insights queries at regular intervals. Scheduled
      # queries enable proactive monitoring by automatically executing queries to detect patterns and
      # anomalies in your log data. Query results can be delivered to Amazon S3 for analysis or further
      # processing.
      def create_scheduled_query(
        execution_role_arn : String,
        name : String,
        query_language : String,
        query_string : String,
        schedule_expression : String,
        description : String? = nil,
        destination_configuration : Types::DestinationConfiguration? = nil,
        log_group_identifiers : Array(String)? = nil,
        schedule_end_time : Int64? = nil,
        schedule_start_time : Int64? = nil,
        start_time_offset : Int64? = nil,
        state : String? = nil,
        tags : Hash(String, String)? = nil,
        timezone : String? = nil
      ) : Types::CreateScheduledQueryResponse
        input = Types::CreateScheduledQueryRequest.new(execution_role_arn: execution_role_arn, name: name, query_language: query_language, query_string: query_string, schedule_expression: schedule_expression, description: description, destination_configuration: destination_configuration, log_group_identifiers: log_group_identifiers, schedule_end_time: schedule_end_time, schedule_start_time: schedule_start_time, start_time_offset: start_time_offset, state: state, tags: tags, timezone: timezone)
        create_scheduled_query(input)
      end
      def create_scheduled_query(input : Types::CreateScheduledQueryRequest) : Types::CreateScheduledQueryResponse
        request = Protocol::JsonRpc.build_request(Model::CREATE_SCHEDULED_QUERY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::CreateScheduledQueryResponse, "CreateScheduledQuery")
      end

      # Deletes a CloudWatch Logs account policy. This stops the account-wide policy from applying to log
      # groups or data sources in the account. If you delete a data protection policy or subscription filter
      # policy, any log-group level policies of those types remain in effect. This operation supports
      # deletion of data source-based field index policies, including facet configurations, in addition to
      # log group-based policies. To use this operation, you must be signed on with the correct permissions
      # depending on the type of policy that you are deleting. To delete a data protection policy, you must
      # have the logs:DeleteDataProtectionPolicy and logs:DeleteAccountPolicy permissions. To delete a
      # subscription filter policy, you must have the logs:DeleteSubscriptionFilter and
      # logs:DeleteAccountPolicy permissions. To delete a transformer policy, you must have the
      # logs:DeleteTransformer and logs:DeleteAccountPolicy permissions. To delete a field index policy, you
      # must have the logs:DeleteIndexPolicy and logs:DeleteAccountPolicy permissions. If you delete a field
      # index policy that included facet configurations, those facets will no longer be available for
      # interactive exploration in the CloudWatch Logs Insights console. However, facet data is retained for
      # up to 30 days. If you delete a field index policy, the indexing of the log events that happened
      # before you deleted the policy will still be used for up to 30 days to improve CloudWatch Logs
      # Insights queries.
      def delete_account_policy(
        policy_name : String,
        policy_type : String
      ) : Nil
        input = Types::DeleteAccountPolicyRequest.new(policy_name: policy_name, policy_type: policy_type)
        delete_account_policy(input)
      end
      def delete_account_policy(input : Types::DeleteAccountPolicyRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::DELETE_ACCOUNT_POLICY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Deletes the data protection policy from the specified log group. For more information about data
      # protection policies, see PutDataProtectionPolicy .
      def delete_data_protection_policy(
        log_group_identifier : String
      ) : Nil
        input = Types::DeleteDataProtectionPolicyRequest.new(log_group_identifier: log_group_identifier)
        delete_data_protection_policy(input)
      end
      def delete_data_protection_policy(input : Types::DeleteDataProtectionPolicyRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::DELETE_DATA_PROTECTION_POLICY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Deletes a delivery . A delivery is a connection between a logical delivery source and a logical
      # delivery destination . Deleting a delivery only deletes the connection between the delivery source
      # and delivery destination. It does not delete the delivery destination or the delivery source.
      def delete_delivery(
        id : String
      ) : Nil
        input = Types::DeleteDeliveryRequest.new(id: id)
        delete_delivery(input)
      end
      def delete_delivery(input : Types::DeleteDeliveryRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::DELETE_DELIVERY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Deletes a delivery destination . A delivery is a connection between a logical delivery source and a
      # logical delivery destination . You can't delete a delivery destination if any current deliveries are
      # associated with it. To find whether any deliveries are associated with this delivery destination,
      # use the DescribeDeliveries operation and check the deliveryDestinationArn field in the results.
      def delete_delivery_destination(
        name : String
      ) : Nil
        input = Types::DeleteDeliveryDestinationRequest.new(name: name)
        delete_delivery_destination(input)
      end
      def delete_delivery_destination(input : Types::DeleteDeliveryDestinationRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::DELETE_DELIVERY_DESTINATION, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Deletes a delivery destination policy. For more information about these policies, see
      # PutDeliveryDestinationPolicy .
      def delete_delivery_destination_policy(
        delivery_destination_name : String
      ) : Nil
        input = Types::DeleteDeliveryDestinationPolicyRequest.new(delivery_destination_name: delivery_destination_name)
        delete_delivery_destination_policy(input)
      end
      def delete_delivery_destination_policy(input : Types::DeleteDeliveryDestinationPolicyRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::DELETE_DELIVERY_DESTINATION_POLICY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Deletes a delivery source . A delivery is a connection between a logical delivery source and a
      # logical delivery destination . You can't delete a delivery source if any current deliveries are
      # associated with it. To find whether any deliveries are associated with this delivery source, use the
      # DescribeDeliveries operation and check the deliverySourceName field in the results.
      def delete_delivery_source(
        name : String
      ) : Nil
        input = Types::DeleteDeliverySourceRequest.new(name: name)
        delete_delivery_source(input)
      end
      def delete_delivery_source(input : Types::DeleteDeliverySourceRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::DELETE_DELIVERY_SOURCE, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Deletes the specified destination, and eventually disables all the subscription filters that publish
      # to it. This operation does not delete the physical resource encapsulated by the destination.
      def delete_destination(
        destination_name : String
      ) : Nil
        input = Types::DeleteDestinationRequest.new(destination_name: destination_name)
        delete_destination(input)
      end
      def delete_destination(input : Types::DeleteDestinationRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::DELETE_DESTINATION, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Deletes a log-group level field index policy that was applied to a single log group. The indexing of
      # the log events that happened before you delete the policy will still be used for as many as 30 days
      # to improve CloudWatch Logs Insights queries. If the deleted policy included facet configurations,
      # those facets will no longer be available for interactive exploration in the CloudWatch Logs Insights
      # console for this log group. However, facet data is retained for up to 30 days. You can't use this
      # operation to delete an account-level index policy. Instead, use DeleteAccountPolicy . If you delete
      # a log-group level field index policy and there is an account-level field index policy, in a few
      # minutes the log group begins using that account-wide policy to index new incoming log events. This
      # operation only affects log group-level policies, including any facet configurations, and preserves
      # any data source-based account policies that may apply to the log group.
      def delete_index_policy(
        log_group_identifier : String
      ) : Types::DeleteIndexPolicyResponse
        input = Types::DeleteIndexPolicyRequest.new(log_group_identifier: log_group_identifier)
        delete_index_policy(input)
      end
      def delete_index_policy(input : Types::DeleteIndexPolicyRequest) : Types::DeleteIndexPolicyResponse
        request = Protocol::JsonRpc.build_request(Model::DELETE_INDEX_POLICY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DeleteIndexPolicyResponse, "DeleteIndexPolicy")
      end

      # Deletes the integration between CloudWatch Logs and OpenSearch Service. If your integration has
      # active vended logs dashboards, you must specify true for the force parameter, otherwise the
      # operation will fail. If you delete the integration by setting force to true , all your vended logs
      # dashboards powered by OpenSearch Service will be deleted and the data that was on them will no
      # longer be accessible.
      def delete_integration(
        integration_name : String,
        force : Bool? = nil
      ) : Types::DeleteIntegrationResponse
        input = Types::DeleteIntegrationRequest.new(integration_name: integration_name, force: force)
        delete_integration(input)
      end
      def delete_integration(input : Types::DeleteIntegrationRequest) : Types::DeleteIntegrationResponse
        request = Protocol::JsonRpc.build_request(Model::DELETE_INTEGRATION, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DeleteIntegrationResponse, "DeleteIntegration")
      end

      # Deletes the specified CloudWatch Logs anomaly detector.
      def delete_log_anomaly_detector(
        anomaly_detector_arn : String
      ) : Nil
        input = Types::DeleteLogAnomalyDetectorRequest.new(anomaly_detector_arn: anomaly_detector_arn)
        delete_log_anomaly_detector(input)
      end
      def delete_log_anomaly_detector(input : Types::DeleteLogAnomalyDetectorRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::DELETE_LOG_ANOMALY_DETECTOR, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Deletes the specified log group and permanently deletes all the archived log events associated with
      # the log group.
      def delete_log_group(
        log_group_name : String
      ) : Nil
        input = Types::DeleteLogGroupRequest.new(log_group_name: log_group_name)
        delete_log_group(input)
      end
      def delete_log_group(input : Types::DeleteLogGroupRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::DELETE_LOG_GROUP, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Deletes the specified log stream and permanently deletes all the archived log events associated with
      # the log stream.
      def delete_log_stream(
        log_group_name : String,
        log_stream_name : String
      ) : Nil
        input = Types::DeleteLogStreamRequest.new(log_group_name: log_group_name, log_stream_name: log_stream_name)
        delete_log_stream(input)
      end
      def delete_log_stream(input : Types::DeleteLogStreamRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::DELETE_LOG_STREAM, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Deletes the specified metric filter.
      def delete_metric_filter(
        filter_name : String,
        log_group_name : String
      ) : Nil
        input = Types::DeleteMetricFilterRequest.new(filter_name: filter_name, log_group_name: log_group_name)
        delete_metric_filter(input)
      end
      def delete_metric_filter(input : Types::DeleteMetricFilterRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::DELETE_METRIC_FILTER, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Deletes a saved CloudWatch Logs Insights query definition. A query definition contains details about
      # a saved CloudWatch Logs Insights query. Each DeleteQueryDefinition operation can delete one query
      # definition. You must have the logs:DeleteQueryDefinition permission to be able to perform this
      # operation.
      def delete_query_definition(
        query_definition_id : String
      ) : Types::DeleteQueryDefinitionResponse
        input = Types::DeleteQueryDefinitionRequest.new(query_definition_id: query_definition_id)
        delete_query_definition(input)
      end
      def delete_query_definition(input : Types::DeleteQueryDefinitionRequest) : Types::DeleteQueryDefinitionResponse
        request = Protocol::JsonRpc.build_request(Model::DELETE_QUERY_DEFINITION, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DeleteQueryDefinitionResponse, "DeleteQueryDefinition")
      end

      # Deletes a resource policy from this account. This revokes the access of the identities in that
      # policy to put log events to this account.
      def delete_resource_policy(
        expected_revision_id : String? = nil,
        policy_name : String? = nil,
        resource_arn : String? = nil
      ) : Nil
        input = Types::DeleteResourcePolicyRequest.new(expected_revision_id: expected_revision_id, policy_name: policy_name, resource_arn: resource_arn)
        delete_resource_policy(input)
      end
      def delete_resource_policy(input : Types::DeleteResourcePolicyRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::DELETE_RESOURCE_POLICY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Deletes the specified retention policy. Log events do not expire if they belong to log groups
      # without a retention policy.
      def delete_retention_policy(
        log_group_name : String
      ) : Nil
        input = Types::DeleteRetentionPolicyRequest.new(log_group_name: log_group_name)
        delete_retention_policy(input)
      end
      def delete_retention_policy(input : Types::DeleteRetentionPolicyRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::DELETE_RETENTION_POLICY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Deletes a scheduled query and stops all future executions. This operation also removes any
      # configured actions and associated resources.
      def delete_scheduled_query(
        identifier : String
      ) : Types::DeleteScheduledQueryResponse
        input = Types::DeleteScheduledQueryRequest.new(identifier: identifier)
        delete_scheduled_query(input)
      end
      def delete_scheduled_query(input : Types::DeleteScheduledQueryRequest) : Types::DeleteScheduledQueryResponse
        request = Protocol::JsonRpc.build_request(Model::DELETE_SCHEDULED_QUERY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DeleteScheduledQueryResponse, "DeleteScheduledQuery")
      end

      # Deletes the specified subscription filter.
      def delete_subscription_filter(
        filter_name : String,
        log_group_name : String
      ) : Nil
        input = Types::DeleteSubscriptionFilterRequest.new(filter_name: filter_name, log_group_name: log_group_name)
        delete_subscription_filter(input)
      end
      def delete_subscription_filter(input : Types::DeleteSubscriptionFilterRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::DELETE_SUBSCRIPTION_FILTER, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Deletes the log transformer for the specified log group. As soon as you do this, the transformation
      # of incoming log events according to that transformer stops. If this account has an account-level
      # transformer that applies to this log group, the log group begins using that account-level
      # transformer when this log-group level transformer is deleted. After you delete a transformer, be
      # sure to edit any metric filters or subscription filters that relied on the transformed versions of
      # the log events.
      def delete_transformer(
        log_group_identifier : String
      ) : Nil
        input = Types::DeleteTransformerRequest.new(log_group_identifier: log_group_identifier)
        delete_transformer(input)
      end
      def delete_transformer(input : Types::DeleteTransformerRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::DELETE_TRANSFORMER, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Returns a list of all CloudWatch Logs account policies in the account. To use this operation, you
      # must be signed on with the correct permissions depending on the type of policy that you are
      # retrieving information for. To see data protection policies, you must have the
      # logs:GetDataProtectionPolicy and logs:DescribeAccountPolicies permissions. To see subscription
      # filter policies, you must have the logs:DescribeSubscriptionFilters and logs:DescribeAccountPolicies
      # permissions. To see transformer policies, you must have the logs:GetTransformer and
      # logs:DescribeAccountPolicies permissions. To see field index policies, you must have the
      # logs:DescribeIndexPolicies and logs:DescribeAccountPolicies permissions.
      def describe_account_policies(
        policy_type : String,
        account_identifiers : Array(String)? = nil,
        next_token : String? = nil,
        policy_name : String? = nil
      ) : Types::DescribeAccountPoliciesResponse
        input = Types::DescribeAccountPoliciesRequest.new(policy_type: policy_type, account_identifiers: account_identifiers, next_token: next_token, policy_name: policy_name)
        describe_account_policies(input)
      end
      def describe_account_policies(input : Types::DescribeAccountPoliciesRequest) : Types::DescribeAccountPoliciesResponse
        request = Protocol::JsonRpc.build_request(Model::DESCRIBE_ACCOUNT_POLICIES, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DescribeAccountPoliciesResponse, "DescribeAccountPolicies")
      end

      # Use this operation to return the valid and default values that are used when creating delivery
      # sources, delivery destinations, and deliveries. For more information about deliveries, see
      # CreateDelivery .
      def describe_configuration_templates(
        delivery_destination_types : Array(String)? = nil,
        limit : Int32? = nil,
        log_types : Array(String)? = nil,
        next_token : String? = nil,
        resource_types : Array(String)? = nil,
        service : String? = nil
      ) : Types::DescribeConfigurationTemplatesResponse
        input = Types::DescribeConfigurationTemplatesRequest.new(delivery_destination_types: delivery_destination_types, limit: limit, log_types: log_types, next_token: next_token, resource_types: resource_types, service: service)
        describe_configuration_templates(input)
      end
      def describe_configuration_templates(input : Types::DescribeConfigurationTemplatesRequest) : Types::DescribeConfigurationTemplatesResponse
        request = Protocol::JsonRpc.build_request(Model::DESCRIBE_CONFIGURATION_TEMPLATES, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DescribeConfigurationTemplatesResponse, "DescribeConfigurationTemplates")
      end

      # Retrieves a list of the deliveries that have been created in the account. A delivery is a connection
      # between a delivery source and a delivery destination . A delivery source represents an Amazon Web
      # Services resource that sends logs to an logs delivery destination. The destination can be CloudWatch
      # Logs, Amazon S3, Firehose or X-Ray. Only some Amazon Web Services services support being configured
      # as a delivery source. These services are listed in Enable logging from Amazon Web Services services.
      def describe_deliveries(
        limit : Int32? = nil,
        next_token : String? = nil
      ) : Types::DescribeDeliveriesResponse
        input = Types::DescribeDeliveriesRequest.new(limit: limit, next_token: next_token)
        describe_deliveries(input)
      end
      def describe_deliveries(input : Types::DescribeDeliveriesRequest) : Types::DescribeDeliveriesResponse
        request = Protocol::JsonRpc.build_request(Model::DESCRIBE_DELIVERIES, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DescribeDeliveriesResponse, "DescribeDeliveries")
      end

      # Retrieves a list of the delivery destinations that have been created in the account.
      def describe_delivery_destinations(
        limit : Int32? = nil,
        next_token : String? = nil
      ) : Types::DescribeDeliveryDestinationsResponse
        input = Types::DescribeDeliveryDestinationsRequest.new(limit: limit, next_token: next_token)
        describe_delivery_destinations(input)
      end
      def describe_delivery_destinations(input : Types::DescribeDeliveryDestinationsRequest) : Types::DescribeDeliveryDestinationsResponse
        request = Protocol::JsonRpc.build_request(Model::DESCRIBE_DELIVERY_DESTINATIONS, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DescribeDeliveryDestinationsResponse, "DescribeDeliveryDestinations")
      end

      # Retrieves a list of the delivery sources that have been created in the account.
      def describe_delivery_sources(
        limit : Int32? = nil,
        next_token : String? = nil
      ) : Types::DescribeDeliverySourcesResponse
        input = Types::DescribeDeliverySourcesRequest.new(limit: limit, next_token: next_token)
        describe_delivery_sources(input)
      end
      def describe_delivery_sources(input : Types::DescribeDeliverySourcesRequest) : Types::DescribeDeliverySourcesResponse
        request = Protocol::JsonRpc.build_request(Model::DESCRIBE_DELIVERY_SOURCES, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DescribeDeliverySourcesResponse, "DescribeDeliverySources")
      end

      # Lists all your destinations. The results are ASCII-sorted by destination name.
      def describe_destinations(
        destination_name_prefix : String? = nil,
        limit : Int32? = nil,
        next_token : String? = nil
      ) : Types::DescribeDestinationsResponse
        input = Types::DescribeDestinationsRequest.new(destination_name_prefix: destination_name_prefix, limit: limit, next_token: next_token)
        describe_destinations(input)
      end
      def describe_destinations(input : Types::DescribeDestinationsRequest) : Types::DescribeDestinationsResponse
        request = Protocol::JsonRpc.build_request(Model::DESCRIBE_DESTINATIONS, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DescribeDestinationsResponse, "DescribeDestinations")
      end

      # Lists the specified export tasks. You can list all your export tasks or filter the results based on
      # task ID or task status.
      def describe_export_tasks(
        limit : Int32? = nil,
        next_token : String? = nil,
        status_code : String? = nil,
        task_id : String? = nil
      ) : Types::DescribeExportTasksResponse
        input = Types::DescribeExportTasksRequest.new(limit: limit, next_token: next_token, status_code: status_code, task_id: task_id)
        describe_export_tasks(input)
      end
      def describe_export_tasks(input : Types::DescribeExportTasksRequest) : Types::DescribeExportTasksResponse
        request = Protocol::JsonRpc.build_request(Model::DESCRIBE_EXPORT_TASKS, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DescribeExportTasksResponse, "DescribeExportTasks")
      end

      # Returns a list of custom and default field indexes which are discovered in log data. For more
      # information about field index policies, see PutIndexPolicy .
      def describe_field_indexes(
        log_group_identifiers : Array(String),
        next_token : String? = nil
      ) : Types::DescribeFieldIndexesResponse
        input = Types::DescribeFieldIndexesRequest.new(log_group_identifiers: log_group_identifiers, next_token: next_token)
        describe_field_indexes(input)
      end
      def describe_field_indexes(input : Types::DescribeFieldIndexesRequest) : Types::DescribeFieldIndexesResponse
        request = Protocol::JsonRpc.build_request(Model::DESCRIBE_FIELD_INDEXES, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DescribeFieldIndexesResponse, "DescribeFieldIndexes")
      end

      # Gets detailed information about the individual batches within an import task, including their status
      # and any error messages. For CloudTrail Event Data Store sources, a batch refers to a subset of
      # stored events grouped by their eventTime.
      def describe_import_task_batches(
        import_id : String,
        batch_import_status : Array(String)? = nil,
        limit : Int32? = nil,
        next_token : String? = nil
      ) : Types::DescribeImportTaskBatchesResponse
        input = Types::DescribeImportTaskBatchesRequest.new(import_id: import_id, batch_import_status: batch_import_status, limit: limit, next_token: next_token)
        describe_import_task_batches(input)
      end
      def describe_import_task_batches(input : Types::DescribeImportTaskBatchesRequest) : Types::DescribeImportTaskBatchesResponse
        request = Protocol::JsonRpc.build_request(Model::DESCRIBE_IMPORT_TASK_BATCHES, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DescribeImportTaskBatchesResponse, "DescribeImportTaskBatches")
      end

      # Lists and describes import tasks, with optional filtering by import status and source ARN.
      def describe_import_tasks(
        import_id : String? = nil,
        import_source_arn : String? = nil,
        import_status : String? = nil,
        limit : Int32? = nil,
        next_token : String? = nil
      ) : Types::DescribeImportTasksResponse
        input = Types::DescribeImportTasksRequest.new(import_id: import_id, import_source_arn: import_source_arn, import_status: import_status, limit: limit, next_token: next_token)
        describe_import_tasks(input)
      end
      def describe_import_tasks(input : Types::DescribeImportTasksRequest) : Types::DescribeImportTasksResponse
        request = Protocol::JsonRpc.build_request(Model::DESCRIBE_IMPORT_TASKS, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DescribeImportTasksResponse, "DescribeImportTasks")
      end

      # Returns the field index policies of the specified log group. For more information about field index
      # policies, see PutIndexPolicy . If a specified log group has a log-group level index policy, that
      # policy is returned by this operation. If a specified log group doesn't have a log-group level index
      # policy, but an account-wide index policy applies to it, that account-wide policy is returned by this
      # operation. To find information about only account-level policies, use DescribeAccountPolicies
      # instead.
      def describe_index_policies(
        log_group_identifiers : Array(String),
        next_token : String? = nil
      ) : Types::DescribeIndexPoliciesResponse
        input = Types::DescribeIndexPoliciesRequest.new(log_group_identifiers: log_group_identifiers, next_token: next_token)
        describe_index_policies(input)
      end
      def describe_index_policies(input : Types::DescribeIndexPoliciesRequest) : Types::DescribeIndexPoliciesResponse
        request = Protocol::JsonRpc.build_request(Model::DESCRIBE_INDEX_POLICIES, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DescribeIndexPoliciesResponse, "DescribeIndexPolicies")
      end

      # Returns information about log groups, including data sources that ingest into each log group. You
      # can return all your log groups or filter the results by prefix. The results are ASCII-sorted by log
      # group name. CloudWatch Logs doesn't support IAM policies that control access to the
      # DescribeLogGroups action by using the aws:ResourceTag/ key-name condition key. Other CloudWatch Logs
      # actions do support the use of the aws:ResourceTag/ key-name condition key to control access. For
      # more information about using tags to control access, see Controlling access to Amazon Web Services
      # resources using tags . If you are using CloudWatch cross-account observability, you can use this
      # operation in a monitoring account and view data from the linked source accounts. For more
      # information, see CloudWatch cross-account observability .
      def describe_log_groups(
        account_identifiers : Array(String)? = nil,
        include_linked_accounts : Bool? = nil,
        limit : Int32? = nil,
        log_group_class : String? = nil,
        log_group_identifiers : Array(String)? = nil,
        log_group_name_pattern : String? = nil,
        log_group_name_prefix : String? = nil,
        next_token : String? = nil
      ) : Types::DescribeLogGroupsResponse
        input = Types::DescribeLogGroupsRequest.new(account_identifiers: account_identifiers, include_linked_accounts: include_linked_accounts, limit: limit, log_group_class: log_group_class, log_group_identifiers: log_group_identifiers, log_group_name_pattern: log_group_name_pattern, log_group_name_prefix: log_group_name_prefix, next_token: next_token)
        describe_log_groups(input)
      end
      def describe_log_groups(input : Types::DescribeLogGroupsRequest) : Types::DescribeLogGroupsResponse
        request = Protocol::JsonRpc.build_request(Model::DESCRIBE_LOG_GROUPS, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DescribeLogGroupsResponse, "DescribeLogGroups")
      end

      # Lists the log streams for the specified log group. You can list all the log streams or filter the
      # results by prefix. You can also control how the results are ordered. You can specify the log group
      # to search by using either logGroupIdentifier or logGroupName . You must include one of these two
      # parameters, but you can't include both. This operation has a limit of 25 transactions per second,
      # after which transactions are throttled. If you are using CloudWatch cross-account observability, you
      # can use this operation in a monitoring account and view data from the linked source accounts. For
      # more information, see CloudWatch cross-account observability .
      def describe_log_streams(
        descending : Bool? = nil,
        limit : Int32? = nil,
        log_group_identifier : String? = nil,
        log_group_name : String? = nil,
        log_stream_name_prefix : String? = nil,
        next_token : String? = nil,
        order_by : String? = nil
      ) : Types::DescribeLogStreamsResponse
        input = Types::DescribeLogStreamsRequest.new(descending: descending, limit: limit, log_group_identifier: log_group_identifier, log_group_name: log_group_name, log_stream_name_prefix: log_stream_name_prefix, next_token: next_token, order_by: order_by)
        describe_log_streams(input)
      end
      def describe_log_streams(input : Types::DescribeLogStreamsRequest) : Types::DescribeLogStreamsResponse
        request = Protocol::JsonRpc.build_request(Model::DESCRIBE_LOG_STREAMS, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DescribeLogStreamsResponse, "DescribeLogStreams")
      end

      # Lists the specified metric filters. You can list all of the metric filters or filter the results by
      # log name, prefix, metric name, or metric namespace. The results are ASCII-sorted by filter name.
      def describe_metric_filters(
        filter_name_prefix : String? = nil,
        limit : Int32? = nil,
        log_group_name : String? = nil,
        metric_name : String? = nil,
        metric_namespace : String? = nil,
        next_token : String? = nil
      ) : Types::DescribeMetricFiltersResponse
        input = Types::DescribeMetricFiltersRequest.new(filter_name_prefix: filter_name_prefix, limit: limit, log_group_name: log_group_name, metric_name: metric_name, metric_namespace: metric_namespace, next_token: next_token)
        describe_metric_filters(input)
      end
      def describe_metric_filters(input : Types::DescribeMetricFiltersRequest) : Types::DescribeMetricFiltersResponse
        request = Protocol::JsonRpc.build_request(Model::DESCRIBE_METRIC_FILTERS, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DescribeMetricFiltersResponse, "DescribeMetricFilters")
      end

      # Returns a list of CloudWatch Logs Insights queries that are scheduled, running, or have been run
      # recently in this account. You can request all queries or limit it to queries of a specific log group
      # or queries with a certain status. This operation includes both interactive queries started directly
      # by users and automated queries executed by scheduled query configurations. Scheduled query
      # executions appear in the results alongside manually initiated queries, providing visibility into all
      # query activity in your account.
      def describe_queries(
        log_group_name : String? = nil,
        max_results : Int32? = nil,
        next_token : String? = nil,
        query_language : String? = nil,
        status : String? = nil
      ) : Types::DescribeQueriesResponse
        input = Types::DescribeQueriesRequest.new(log_group_name: log_group_name, max_results: max_results, next_token: next_token, query_language: query_language, status: status)
        describe_queries(input)
      end
      def describe_queries(input : Types::DescribeQueriesRequest) : Types::DescribeQueriesResponse
        request = Protocol::JsonRpc.build_request(Model::DESCRIBE_QUERIES, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DescribeQueriesResponse, "DescribeQueries")
      end

      # This operation returns a paginated list of your saved CloudWatch Logs Insights query definitions.
      # You can retrieve query definitions from the current account or from a source account that is linked
      # to the current account. You can use the queryDefinitionNamePrefix parameter to limit the results to
      # only the query definitions that have names that start with a certain string.
      def describe_query_definitions(
        max_results : Int32? = nil,
        next_token : String? = nil,
        query_definition_name_prefix : String? = nil,
        query_language : String? = nil
      ) : Types::DescribeQueryDefinitionsResponse
        input = Types::DescribeQueryDefinitionsRequest.new(max_results: max_results, next_token: next_token, query_definition_name_prefix: query_definition_name_prefix, query_language: query_language)
        describe_query_definitions(input)
      end
      def describe_query_definitions(input : Types::DescribeQueryDefinitionsRequest) : Types::DescribeQueryDefinitionsResponse
        request = Protocol::JsonRpc.build_request(Model::DESCRIBE_QUERY_DEFINITIONS, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DescribeQueryDefinitionsResponse, "DescribeQueryDefinitions")
      end

      # Lists the resource policies in this account.
      def describe_resource_policies(
        limit : Int32? = nil,
        next_token : String? = nil,
        policy_scope : String? = nil,
        resource_arn : String? = nil
      ) : Types::DescribeResourcePoliciesResponse
        input = Types::DescribeResourcePoliciesRequest.new(limit: limit, next_token: next_token, policy_scope: policy_scope, resource_arn: resource_arn)
        describe_resource_policies(input)
      end
      def describe_resource_policies(input : Types::DescribeResourcePoliciesRequest) : Types::DescribeResourcePoliciesResponse
        request = Protocol::JsonRpc.build_request(Model::DESCRIBE_RESOURCE_POLICIES, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DescribeResourcePoliciesResponse, "DescribeResourcePolicies")
      end

      # Lists the subscription filters for the specified log group. You can list all the subscription
      # filters or filter the results by prefix. The results are ASCII-sorted by filter name.
      def describe_subscription_filters(
        log_group_name : String,
        filter_name_prefix : String? = nil,
        limit : Int32? = nil,
        next_token : String? = nil
      ) : Types::DescribeSubscriptionFiltersResponse
        input = Types::DescribeSubscriptionFiltersRequest.new(log_group_name: log_group_name, filter_name_prefix: filter_name_prefix, limit: limit, next_token: next_token)
        describe_subscription_filters(input)
      end
      def describe_subscription_filters(input : Types::DescribeSubscriptionFiltersRequest) : Types::DescribeSubscriptionFiltersResponse
        request = Protocol::JsonRpc.build_request(Model::DESCRIBE_SUBSCRIPTION_FILTERS, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DescribeSubscriptionFiltersResponse, "DescribeSubscriptionFilters")
      end

      # Disassociates the specified KMS key from the specified log group or from all CloudWatch Logs
      # Insights query results in the account. When you use DisassociateKmsKey , you specify either the
      # logGroupName parameter or the resourceIdentifier parameter. You can't specify both of those
      # parameters in the same operation. Specify the logGroupName parameter to stop using the KMS key to
      # encrypt future log events ingested and stored in the log group. Instead, they will be encrypted with
      # the default CloudWatch Logs method. The log events that were ingested while the key was associated
      # with the log group are still encrypted with that key. Therefore, CloudWatch Logs will need
      # permissions for the key whenever that data is accessed. Specify the resourceIdentifier parameter
      # with the query-result resource to stop using the KMS key to encrypt the results of all future
      # StartQuery operations in the account. They will instead be encrypted with the default CloudWatch
      # Logs method. The results from queries that ran while the key was associated with the account are
      # still encrypted with that key. Therefore, CloudWatch Logs will need permissions for the key whenever
      # that data is accessed. It can take up to 5 minutes for this operation to take effect.
      def disassociate_kms_key(
        log_group_name : String? = nil,
        resource_identifier : String? = nil
      ) : Nil
        input = Types::DisassociateKmsKeyRequest.new(log_group_name: log_group_name, resource_identifier: resource_identifier)
        disassociate_kms_key(input)
      end
      def disassociate_kms_key(input : Types::DisassociateKmsKeyRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::DISASSOCIATE_KMS_KEY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Disassociates a data source from an S3 Table Integration, removing query access and deleting all
      # associated data from the integration.
      def disassociate_source_from_s3_table_integration(
        identifier : String
      ) : Types::DisassociateSourceFromS3TableIntegrationResponse
        input = Types::DisassociateSourceFromS3TableIntegrationRequest.new(identifier: identifier)
        disassociate_source_from_s3_table_integration(input)
      end
      def disassociate_source_from_s3_table_integration(input : Types::DisassociateSourceFromS3TableIntegrationRequest) : Types::DisassociateSourceFromS3TableIntegrationResponse
        request = Protocol::JsonRpc.build_request(Model::DISASSOCIATE_SOURCE_FROM_S3_TABLE_INTEGRATION, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::DisassociateSourceFromS3TableIntegrationResponse, "DisassociateSourceFromS3TableIntegration")
      end

      # Lists log events from the specified log group. You can list all the log events or filter the results
      # using one or more of the following: A filter pattern A time range The log stream name, or a log
      # stream name prefix that matches multiple log streams You must have the logs:FilterLogEvents
      # permission to perform this operation. You can specify the log group to search by using either
      # logGroupIdentifier or logGroupName . You must include one of these two parameters, but you can't
      # include both. FilterLogEvents is a paginated operation. Each page returned can contain up to 1 MB of
      # log events or up to 10,000 log events. A returned page might only be partially full, or even empty.
      # For example, if the result of a query would return 15,000 log events, the first page isn't
      # guaranteed to have 10,000 log events even if they all fit into 1 MB. Partially full or empty pages
      # don't necessarily mean that pagination is finished. If the results include a nextToken , there might
      # be more log events available. You can return these additional log events by providing the nextToken
      # in a subsequent FilterLogEvents operation. If the results don't include a nextToken , then
      # pagination is finished. Specifying the limit parameter only guarantees that a single page doesn't
      # return more log events than the specified limit, but it might return fewer events than the limit.
      # This is the expected API behavior. The returned log events are sorted by event timestamp, the
      # timestamp when the event was ingested by CloudWatch Logs, and the ID of the PutLogEvents request. If
      # you are using CloudWatch cross-account observability, you can use this operation in a monitoring
      # account and view data from the linked source accounts. For more information, see CloudWatch
      # cross-account observability . If you are using log transformation , the FilterLogEvents operation
      # returns only the original versions of log events, before they were transformed. To view the
      # transformed versions, you must use a CloudWatch Logs query.
      def filter_log_events(
        end_time : Int64? = nil,
        filter_pattern : String? = nil,
        interleaved : Bool? = nil,
        limit : Int32? = nil,
        log_group_identifier : String? = nil,
        log_group_name : String? = nil,
        log_stream_name_prefix : String? = nil,
        log_stream_names : Array(String)? = nil,
        next_token : String? = nil,
        start_time : Int64? = nil,
        unmask : Bool? = nil
      ) : Types::FilterLogEventsResponse
        input = Types::FilterLogEventsRequest.new(end_time: end_time, filter_pattern: filter_pattern, interleaved: interleaved, limit: limit, log_group_identifier: log_group_identifier, log_group_name: log_group_name, log_stream_name_prefix: log_stream_name_prefix, log_stream_names: log_stream_names, next_token: next_token, start_time: start_time, unmask: unmask)
        filter_log_events(input)
      end
      def filter_log_events(input : Types::FilterLogEventsRequest) : Types::FilterLogEventsResponse
        request = Protocol::JsonRpc.build_request(Model::FILTER_LOG_EVENTS, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::FilterLogEventsResponse, "FilterLogEvents")
      end

      # Returns information about a log group data protection policy.
      def get_data_protection_policy(
        log_group_identifier : String
      ) : Types::GetDataProtectionPolicyResponse
        input = Types::GetDataProtectionPolicyRequest.new(log_group_identifier: log_group_identifier)
        get_data_protection_policy(input)
      end
      def get_data_protection_policy(input : Types::GetDataProtectionPolicyRequest) : Types::GetDataProtectionPolicyResponse
        request = Protocol::JsonRpc.build_request(Model::GET_DATA_PROTECTION_POLICY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::GetDataProtectionPolicyResponse, "GetDataProtectionPolicy")
      end

      # Returns complete information about one logical delivery . A delivery is a connection between a
      # delivery source and a delivery destination . A delivery source represents an Amazon Web Services
      # resource that sends logs to an logs delivery destination. The destination can be CloudWatch Logs,
      # Amazon S3, or Firehose. Only some Amazon Web Services services support being configured as a
      # delivery source. These services are listed in Enable logging from Amazon Web Services services. You
      # need to specify the delivery id in this operation. You can find the IDs of the deliveries in your
      # account with the DescribeDeliveries operation.
      def get_delivery(
        id : String
      ) : Types::GetDeliveryResponse
        input = Types::GetDeliveryRequest.new(id: id)
        get_delivery(input)
      end
      def get_delivery(input : Types::GetDeliveryRequest) : Types::GetDeliveryResponse
        request = Protocol::JsonRpc.build_request(Model::GET_DELIVERY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::GetDeliveryResponse, "GetDelivery")
      end

      # Retrieves complete information about one delivery destination.
      def get_delivery_destination(
        name : String
      ) : Types::GetDeliveryDestinationResponse
        input = Types::GetDeliveryDestinationRequest.new(name: name)
        get_delivery_destination(input)
      end
      def get_delivery_destination(input : Types::GetDeliveryDestinationRequest) : Types::GetDeliveryDestinationResponse
        request = Protocol::JsonRpc.build_request(Model::GET_DELIVERY_DESTINATION, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::GetDeliveryDestinationResponse, "GetDeliveryDestination")
      end

      # Retrieves the delivery destination policy assigned to the delivery destination that you specify. For
      # more information about delivery destinations and their policies, see PutDeliveryDestinationPolicy .
      def get_delivery_destination_policy(
        delivery_destination_name : String
      ) : Types::GetDeliveryDestinationPolicyResponse
        input = Types::GetDeliveryDestinationPolicyRequest.new(delivery_destination_name: delivery_destination_name)
        get_delivery_destination_policy(input)
      end
      def get_delivery_destination_policy(input : Types::GetDeliveryDestinationPolicyRequest) : Types::GetDeliveryDestinationPolicyResponse
        request = Protocol::JsonRpc.build_request(Model::GET_DELIVERY_DESTINATION_POLICY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::GetDeliveryDestinationPolicyResponse, "GetDeliveryDestinationPolicy")
      end

      # Retrieves complete information about one delivery source.
      def get_delivery_source(
        name : String
      ) : Types::GetDeliverySourceResponse
        input = Types::GetDeliverySourceRequest.new(name: name)
        get_delivery_source(input)
      end
      def get_delivery_source(input : Types::GetDeliverySourceRequest) : Types::GetDeliverySourceResponse
        request = Protocol::JsonRpc.build_request(Model::GET_DELIVERY_SOURCE, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::GetDeliverySourceResponse, "GetDeliverySource")
      end

      # Returns information about one integration between CloudWatch Logs and OpenSearch Service.
      def get_integration(
        integration_name : String
      ) : Types::GetIntegrationResponse
        input = Types::GetIntegrationRequest.new(integration_name: integration_name)
        get_integration(input)
      end
      def get_integration(input : Types::GetIntegrationRequest) : Types::GetIntegrationResponse
        request = Protocol::JsonRpc.build_request(Model::GET_INTEGRATION, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::GetIntegrationResponse, "GetIntegration")
      end

      # Retrieves information about the log anomaly detector that you specify. The KMS key ARN detected is
      # valid.
      def get_log_anomaly_detector(
        anomaly_detector_arn : String
      ) : Types::GetLogAnomalyDetectorResponse
        input = Types::GetLogAnomalyDetectorRequest.new(anomaly_detector_arn: anomaly_detector_arn)
        get_log_anomaly_detector(input)
      end
      def get_log_anomaly_detector(input : Types::GetLogAnomalyDetectorRequest) : Types::GetLogAnomalyDetectorResponse
        request = Protocol::JsonRpc.build_request(Model::GET_LOG_ANOMALY_DETECTOR, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::GetLogAnomalyDetectorResponse, "GetLogAnomalyDetector")
      end

      # Lists log events from the specified log stream. You can list all of the log events or filter using a
      # time range. GetLogEvents is a paginated operation. Each page returned can contain up to 1 MB of log
      # events or up to 10,000 log events. A returned page might only be partially full, or even empty. For
      # example, if the result of a query would return 15,000 log events, the first page isn't guaranteed to
      # have 10,000 log events even if they all fit into 1 MB. Partially full or empty pages don't
      # necessarily mean that pagination is finished. As long as the nextBackwardToken or nextForwardToken
      # returned is NOT equal to the nextToken that you passed into the API call, there might be more log
      # events available. The token that you use depends on the direction you want to move in along the log
      # stream. The returned tokens are never null. If you set startFromHead to true and you dont include
      # endTime in your request, you can end up in a situation where the pagination doesn't terminate. This
      # can happen when the new log events are being added to the target log streams faster than they are
      # being read. This situation is a good use case for the CloudWatch Logs Live Tail feature. If you are
      # using CloudWatch cross-account observability, you can use this operation in a monitoring account and
      # view data from the linked source accounts. For more information, see CloudWatch cross-account
      # observability . You can specify the log group to search by using either logGroupIdentifier or
      # logGroupName . You must include one of these two parameters, but you can't include both. If you are
      # using log transformation , the GetLogEvents operation returns only the original versions of log
      # events, before they were transformed. To view the transformed versions, you must use a CloudWatch
      # Logs query.
      def get_log_events(
        log_stream_name : String,
        end_time : Int64? = nil,
        limit : Int32? = nil,
        log_group_identifier : String? = nil,
        log_group_name : String? = nil,
        next_token : String? = nil,
        start_from_head : Bool? = nil,
        start_time : Int64? = nil,
        unmask : Bool? = nil
      ) : Types::GetLogEventsResponse
        input = Types::GetLogEventsRequest.new(log_stream_name: log_stream_name, end_time: end_time, limit: limit, log_group_identifier: log_group_identifier, log_group_name: log_group_name, next_token: next_token, start_from_head: start_from_head, start_time: start_time, unmask: unmask)
        get_log_events(input)
      end
      def get_log_events(input : Types::GetLogEventsRequest) : Types::GetLogEventsResponse
        request = Protocol::JsonRpc.build_request(Model::GET_LOG_EVENTS, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::GetLogEventsResponse, "GetLogEvents")
      end

      # Discovers available fields for a specific data source and type. The response includes any field
      # modifications introduced through pipelines, such as new fields or changed field types.
      def get_log_fields(
        data_source_name : String,
        data_source_type : String
      ) : Types::GetLogFieldsResponse
        input = Types::GetLogFieldsRequest.new(data_source_name: data_source_name, data_source_type: data_source_type)
        get_log_fields(input)
      end
      def get_log_fields(input : Types::GetLogFieldsRequest) : Types::GetLogFieldsResponse
        request = Protocol::JsonRpc.build_request(Model::GET_LOG_FIELDS, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::GetLogFieldsResponse, "GetLogFields")
      end

      # Returns a list of the fields that are included in log events in the specified log group. Includes
      # the percentage of log events that contain each field. The search is limited to a time period that
      # you specify. This operation is used for discovering fields within log group events. For discovering
      # fields across data sources, use the GetLogFields operation. You can specify the log group to search
      # by using either logGroupIdentifier or logGroupName . You must specify one of these parameters, but
      # you can't specify both. In the results, fields that start with @ are fields generated by CloudWatch
      # Logs. For example, @timestamp is the timestamp of each log event. For more information about the
      # fields that are generated by CloudWatch logs, see Supported Logs and Discovered Fields . The
      # response results are sorted by the frequency percentage, starting with the highest percentage. If
      # you are using CloudWatch cross-account observability, you can use this operation in a monitoring
      # account and view data from the linked source accounts. For more information, see CloudWatch
      # cross-account observability .
      def get_log_group_fields(
        log_group_identifier : String? = nil,
        log_group_name : String? = nil,
        time : Int64? = nil
      ) : Types::GetLogGroupFieldsResponse
        input = Types::GetLogGroupFieldsRequest.new(log_group_identifier: log_group_identifier, log_group_name: log_group_name, time: time)
        get_log_group_fields(input)
      end
      def get_log_group_fields(input : Types::GetLogGroupFieldsRequest) : Types::GetLogGroupFieldsResponse
        request = Protocol::JsonRpc.build_request(Model::GET_LOG_GROUP_FIELDS, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::GetLogGroupFieldsResponse, "GetLogGroupFields")
      end

      # Retrieves a large logging object (LLO) and streams it back. This API is used to fetch the content of
      # large portions of log events that have been ingested through the PutOpenTelemetryLogs API. When log
      # events contain fields that would cause the total event size to exceed 1MB, CloudWatch Logs
      # automatically processes up to 10 fields, starting with the largest fields. Each field is truncated
      # as needed to keep the total event size as close to 1MB as possible. The excess portions are stored
      # as Large Log Objects (LLOs) and these fields are processed separately and LLO reference system
      # fields (in the format @ptr.$[path.to.field] ) are added. The path in the reference field reflects
      # the original JSON structure where the large field was located. For example, this could be
      # @ptr.$['input']['message'] , @ptr.$['AAA']['BBB']['CCC']['DDD'] , @ptr.$['AAA'] , or any other path
      # matching your log structure.
      def get_log_object(
        log_object_pointer : String,
        unmask : Bool? = nil
      ) : Types::GetLogObjectResponse
        input = Types::GetLogObjectRequest.new(log_object_pointer: log_object_pointer, unmask: unmask)
        get_log_object(input)
      end
      def get_log_object(input : Types::GetLogObjectRequest) : Types::GetLogObjectResponse
        request = Protocol::JsonRpc.build_request(Model::GET_LOG_OBJECT, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::GetLogObjectResponse, "GetLogObject")
      end

      # Retrieves all of the fields and values of a single log event. All fields are retrieved, even if the
      # original query that produced the logRecordPointer retrieved only a subset of fields. Fields are
      # returned as field name/field value pairs. The full unparsed log event is returned within @message .
      def get_log_record(
        log_record_pointer : String,
        unmask : Bool? = nil
      ) : Types::GetLogRecordResponse
        input = Types::GetLogRecordRequest.new(log_record_pointer: log_record_pointer, unmask: unmask)
        get_log_record(input)
      end
      def get_log_record(input : Types::GetLogRecordRequest) : Types::GetLogRecordResponse
        request = Protocol::JsonRpc.build_request(Model::GET_LOG_RECORD, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::GetLogRecordResponse, "GetLogRecord")
      end

      # Returns the results from the specified query. Only the fields requested in the query are returned,
      # along with a @ptr field, which is the identifier for the log record. You can use the value of @ptr
      # in a GetLogRecord operation to get the full log record. GetQueryResults does not start running a
      # query. To run a query, use StartQuery . For more information about how long results of previous
      # queries are available, see CloudWatch Logs quotas . If the value of the Status field in the output
      # is Running , this operation returns only partial results. If you see a value of Scheduled or Running
      # for the status, you can retry the operation later to see the final results. This operation is used
      # both for retrieving results from interactive queries and from automated scheduled query executions.
      # Scheduled queries use GetQueryResults internally to retrieve query results for processing and
      # delivery to configured destinations. If you are using CloudWatch cross-account observability, you
      # can use this operation in a monitoring account to start queries in linked source accounts. For more
      # information, see CloudWatch cross-account observability .
      def get_query_results(
        query_id : String
      ) : Types::GetQueryResultsResponse
        input = Types::GetQueryResultsRequest.new(query_id: query_id)
        get_query_results(input)
      end
      def get_query_results(input : Types::GetQueryResultsRequest) : Types::GetQueryResultsResponse
        request = Protocol::JsonRpc.build_request(Model::GET_QUERY_RESULTS, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::GetQueryResultsResponse, "GetQueryResults")
      end

      # Retrieves details about a specific scheduled query, including its configuration, execution status,
      # and metadata.
      def get_scheduled_query(
        identifier : String
      ) : Types::GetScheduledQueryResponse
        input = Types::GetScheduledQueryRequest.new(identifier: identifier)
        get_scheduled_query(input)
      end
      def get_scheduled_query(input : Types::GetScheduledQueryRequest) : Types::GetScheduledQueryResponse
        request = Protocol::JsonRpc.build_request(Model::GET_SCHEDULED_QUERY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::GetScheduledQueryResponse, "GetScheduledQuery")
      end

      # Retrieves the execution history of a scheduled query within a specified time range, including query
      # results and destination processing status.
      def get_scheduled_query_history(
        end_time : Int64,
        identifier : String,
        start_time : Int64,
        execution_statuses : Array(String)? = nil,
        max_results : Int32? = nil,
        next_token : String? = nil
      ) : Types::GetScheduledQueryHistoryResponse
        input = Types::GetScheduledQueryHistoryRequest.new(end_time: end_time, identifier: identifier, start_time: start_time, execution_statuses: execution_statuses, max_results: max_results, next_token: next_token)
        get_scheduled_query_history(input)
      end
      def get_scheduled_query_history(input : Types::GetScheduledQueryHistoryRequest) : Types::GetScheduledQueryHistoryResponse
        request = Protocol::JsonRpc.build_request(Model::GET_SCHEDULED_QUERY_HISTORY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::GetScheduledQueryHistoryResponse, "GetScheduledQueryHistory")
      end

      # Returns the information about the log transformer associated with this log group. This operation
      # returns data only for transformers created at the log group level. To get information for an
      # account-level transformer, use DescribeAccountPolicies .
      def get_transformer(
        log_group_identifier : String
      ) : Types::GetTransformerResponse
        input = Types::GetTransformerRequest.new(log_group_identifier: log_group_identifier)
        get_transformer(input)
      end
      def get_transformer(input : Types::GetTransformerRequest) : Types::GetTransformerResponse
        request = Protocol::JsonRpc.build_request(Model::GET_TRANSFORMER, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::GetTransformerResponse, "GetTransformer")
      end

      # Returns an aggregate summary of all log groups in the Region grouped by specified data source
      # characteristics. Supports optional filtering by log group class, name patterns, and data sources. If
      # you perform this action in a monitoring account, you can also return aggregated summaries of log
      # groups from source accounts that are linked to the monitoring account. For more information about
      # using cross-account observability to set up monitoring accounts and source accounts, see CloudWatch
      # cross-account observability . The operation aggregates log groups by data source name and type and
      # optionally format, providing counts of log groups that share these characteristics. The operation
      # paginates results. By default, it returns up to 50 results and includes a token to retrieve more
      # results.
      def list_aggregate_log_group_summaries(
        group_by : String,
        account_identifiers : Array(String)? = nil,
        data_sources : Array(Types::DataSourceFilter)? = nil,
        include_linked_accounts : Bool? = nil,
        limit : Int32? = nil,
        log_group_class : String? = nil,
        log_group_name_pattern : String? = nil,
        next_token : String? = nil
      ) : Types::ListAggregateLogGroupSummariesResponse
        input = Types::ListAggregateLogGroupSummariesRequest.new(group_by: group_by, account_identifiers: account_identifiers, data_sources: data_sources, include_linked_accounts: include_linked_accounts, limit: limit, log_group_class: log_group_class, log_group_name_pattern: log_group_name_pattern, next_token: next_token)
        list_aggregate_log_group_summaries(input)
      end
      def list_aggregate_log_group_summaries(input : Types::ListAggregateLogGroupSummariesRequest) : Types::ListAggregateLogGroupSummariesResponse
        request = Protocol::JsonRpc.build_request(Model::LIST_AGGREGATE_LOG_GROUP_SUMMARIES, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::ListAggregateLogGroupSummariesResponse, "ListAggregateLogGroupSummaries")
      end

      # Returns a list of anomalies that log anomaly detectors have found. For details about the structure
      # format of each anomaly object that is returned, see the example in this section.
      def list_anomalies(
        anomaly_detector_arn : String? = nil,
        limit : Int32? = nil,
        next_token : String? = nil,
        suppression_state : String? = nil
      ) : Types::ListAnomaliesResponse
        input = Types::ListAnomaliesRequest.new(anomaly_detector_arn: anomaly_detector_arn, limit: limit, next_token: next_token, suppression_state: suppression_state)
        list_anomalies(input)
      end
      def list_anomalies(input : Types::ListAnomaliesRequest) : Types::ListAnomaliesResponse
        request = Protocol::JsonRpc.build_request(Model::LIST_ANOMALIES, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::ListAnomaliesResponse, "ListAnomalies")
      end

      # Returns a list of integrations between CloudWatch Logs and other services in this account.
      # Currently, only one integration can be created in an account, and this integration must be with
      # OpenSearch Service.
      def list_integrations(
        integration_name_prefix : String? = nil,
        integration_status : String? = nil,
        integration_type : String? = nil
      ) : Types::ListIntegrationsResponse
        input = Types::ListIntegrationsRequest.new(integration_name_prefix: integration_name_prefix, integration_status: integration_status, integration_type: integration_type)
        list_integrations(input)
      end
      def list_integrations(input : Types::ListIntegrationsRequest) : Types::ListIntegrationsResponse
        request = Protocol::JsonRpc.build_request(Model::LIST_INTEGRATIONS, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::ListIntegrationsResponse, "ListIntegrations")
      end

      # Retrieves a list of the log anomaly detectors in the account.
      def list_log_anomaly_detectors(
        filter_log_group_arn : String? = nil,
        limit : Int32? = nil,
        next_token : String? = nil
      ) : Types::ListLogAnomalyDetectorsResponse
        input = Types::ListLogAnomalyDetectorsRequest.new(filter_log_group_arn: filter_log_group_arn, limit: limit, next_token: next_token)
        list_log_anomaly_detectors(input)
      end
      def list_log_anomaly_detectors(input : Types::ListLogAnomalyDetectorsRequest) : Types::ListLogAnomalyDetectorsResponse
        request = Protocol::JsonRpc.build_request(Model::LIST_LOG_ANOMALY_DETECTORS, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::ListLogAnomalyDetectorsResponse, "ListLogAnomalyDetectors")
      end

      # Returns a list of log groups in the Region in your account. If you are performing this action in a
      # monitoring account, you can choose to also return log groups from source accounts that are linked to
      # the monitoring account. For more information about using cross-account observability to set up
      # monitoring accounts and source accounts, see CloudWatch cross-account observability . You can
      # optionally filter the list by log group class, by using regular expressions in your request to match
      # strings in the log group names, by using the fieldIndexes parameter to filter log groups based on
      # which field indexes are configured, by using the dataSources parameter to filter log groups by data
      # source types, and by using the fieldIndexNames parameter to filter by specific field index names.
      # This operation is paginated. By default, your first use of this operation returns 50 results, and
      # includes a token to use in a subsequent operation to return more results.
      def list_log_groups(
        account_identifiers : Array(String)? = nil,
        data_sources : Array(Types::DataSourceFilter)? = nil,
        field_index_names : Array(String)? = nil,
        include_linked_accounts : Bool? = nil,
        limit : Int32? = nil,
        log_group_class : String? = nil,
        log_group_name_pattern : String? = nil,
        next_token : String? = nil
      ) : Types::ListLogGroupsResponse
        input = Types::ListLogGroupsRequest.new(account_identifiers: account_identifiers, data_sources: data_sources, field_index_names: field_index_names, include_linked_accounts: include_linked_accounts, limit: limit, log_group_class: log_group_class, log_group_name_pattern: log_group_name_pattern, next_token: next_token)
        list_log_groups(input)
      end
      def list_log_groups(input : Types::ListLogGroupsRequest) : Types::ListLogGroupsResponse
        request = Protocol::JsonRpc.build_request(Model::LIST_LOG_GROUPS, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::ListLogGroupsResponse, "ListLogGroups")
      end

      # Returns a list of the log groups that were analyzed during a single CloudWatch Logs Insights query.
      # This can be useful for queries that use log group name prefixes or the filterIndex command, because
      # the log groups are dynamically selected in these cases. For more information about field indexes,
      # see Create field indexes to improve query performance and reduce costs .
      def list_log_groups_for_query(
        query_id : String,
        max_results : Int32? = nil,
        next_token : String? = nil
      ) : Types::ListLogGroupsForQueryResponse
        input = Types::ListLogGroupsForQueryRequest.new(query_id: query_id, max_results: max_results, next_token: next_token)
        list_log_groups_for_query(input)
      end
      def list_log_groups_for_query(input : Types::ListLogGroupsForQueryRequest) : Types::ListLogGroupsForQueryResponse
        request = Protocol::JsonRpc.build_request(Model::LIST_LOG_GROUPS_FOR_QUERY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::ListLogGroupsForQueryResponse, "ListLogGroupsForQuery")
      end

      # Lists all scheduled queries in your account and region. You can filter results by state to show only
      # enabled or disabled queries.
      def list_scheduled_queries(
        max_results : Int32? = nil,
        next_token : String? = nil,
        state : String? = nil
      ) : Types::ListScheduledQueriesResponse
        input = Types::ListScheduledQueriesRequest.new(max_results: max_results, next_token: next_token, state: state)
        list_scheduled_queries(input)
      end
      def list_scheduled_queries(input : Types::ListScheduledQueriesRequest) : Types::ListScheduledQueriesResponse
        request = Protocol::JsonRpc.build_request(Model::LIST_SCHEDULED_QUERIES, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::ListScheduledQueriesResponse, "ListScheduledQueries")
      end

      # Returns a list of data source associations for a specified S3 Table Integration, showing which data
      # sources are currently associated for query access.
      def list_sources_for_s3_table_integration(
        integration_arn : String,
        max_results : Int32? = nil,
        next_token : String? = nil
      ) : Types::ListSourcesForS3TableIntegrationResponse
        input = Types::ListSourcesForS3TableIntegrationRequest.new(integration_arn: integration_arn, max_results: max_results, next_token: next_token)
        list_sources_for_s3_table_integration(input)
      end
      def list_sources_for_s3_table_integration(input : Types::ListSourcesForS3TableIntegrationRequest) : Types::ListSourcesForS3TableIntegrationResponse
        request = Protocol::JsonRpc.build_request(Model::LIST_SOURCES_FOR_S3_TABLE_INTEGRATION, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::ListSourcesForS3TableIntegrationResponse, "ListSourcesForS3TableIntegration")
      end

      # Displays the tags associated with a CloudWatch Logs resource. Currently, log groups and destinations
      # support tagging.
      def list_tags_for_resource(
        resource_arn : String
      ) : Types::ListTagsForResourceResponse
        input = Types::ListTagsForResourceRequest.new(resource_arn: resource_arn)
        list_tags_for_resource(input)
      end
      def list_tags_for_resource(input : Types::ListTagsForResourceRequest) : Types::ListTagsForResourceResponse
        request = Protocol::JsonRpc.build_request(Model::LIST_TAGS_FOR_RESOURCE, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::ListTagsForResourceResponse, "ListTagsForResource")
      end

      # The ListTagsLogGroup operation is on the path to deprecation. We recommend that you use
      # ListTagsForResource instead. Lists the tags for the specified log group.
      def list_tags_log_group(
        log_group_name : String
      ) : Types::ListTagsLogGroupResponse
        input = Types::ListTagsLogGroupRequest.new(log_group_name: log_group_name)
        list_tags_log_group(input)
      end
      def list_tags_log_group(input : Types::ListTagsLogGroupRequest) : Types::ListTagsLogGroupResponse
        request = Protocol::JsonRpc.build_request(Model::LIST_TAGS_LOG_GROUP, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::ListTagsLogGroupResponse, "ListTagsLogGroup")
      end

      # Creates an account-level data protection policy, subscription filter policy, field index policy,
      # transformer policy, or metric extraction policy that applies to all log groups, a subset of log
      # groups, or a data source name and type combination in the account. For field index policies, you can
      # configure indexed fields as facets to enable interactive exploration of your logs. Facets provide
      # value distributions and counts for indexed fields in the CloudWatch Logs Insights console without
      # requiring query execution. For more information, see Use facets to group and explore logs . To use
      # this operation, you must be signed on with the correct permissions depending on the type of policy
      # that you are creating. To create a data protection policy, you must have the
      # logs:PutDataProtectionPolicy and logs:PutAccountPolicy permissions. To create a subscription filter
      # policy, you must have the logs:PutSubscriptionFilter and logs:PutAccountPolicy permissions. To
      # create a transformer policy, you must have the logs:PutTransformer and logs:PutAccountPolicy
      # permissions. To create a field index policy, you must have the logs:PutIndexPolicy and
      # logs:PutAccountPolicy permissions. To configure facets for field index policies, you must have the
      # logs:PutIndexPolicy and logs:PutAccountPolicy permissions. To create a metric extraction policy, you
      # must have the logs:PutMetricExtractionPolicy and logs:PutAccountPolicy permissions. Data protection
      # policy A data protection policy can help safeguard sensitive data that's ingested by your log groups
      # by auditing and masking the sensitive log data. Each account can have only one account-level data
      # protection policy. Sensitive data is detected and masked when it is ingested into a log group. When
      # you set a data protection policy, log events ingested into the log groups before that time are not
      # masked. If you use PutAccountPolicy to create a data protection policy for your whole account, it
      # applies to both existing log groups and all log groups that are created later in this account. The
      # account-level policy is applied to existing log groups with eventual consistency. It might take up
      # to 5 minutes before sensitive data in existing log groups begins to be masked. By default, when a
      # user views a log event that includes masked data, the sensitive data is replaced by asterisks. A
      # user who has the logs:Unmask permission can use a GetLogEvents or FilterLogEvents operation with the
      # unmask parameter set to true to view the unmasked log events. Users with the logs:Unmask can also
      # view unmasked data in the CloudWatch Logs console by running a CloudWatch Logs Insights query with
      # the unmask query command. For more information, including a list of types of data that can be
      # audited and masked, see Protect sensitive log data with masking . To use the PutAccountPolicy
      # operation for a data protection policy, you must be signed on with the logs:PutDataProtectionPolicy
      # and logs:PutAccountPolicy permissions. The PutAccountPolicy operation applies to all log groups in
      # the account. You can use PutDataProtectionPolicy to create a data protection policy that applies to
      # just one log group. If a log group has its own data protection policy and the account also has an
      # account-level data protection policy, then the two policies are cumulative. Any sensitive term
      # specified in either policy is masked. Subscription filter policy A subscription filter policy sets
      # up a real-time feed of log events from CloudWatch Logs to other Amazon Web Services services.
      # Account-level subscription filter policies apply to both existing log groups and log groups that are
      # created later in this account. Supported destinations are Kinesis Data Streams, Firehose, and
      # Lambda. When log events are sent to the receiving service, they are Base64 encoded and compressed
      # with the GZIP format. The following destinations are supported for subscription filters: An Kinesis
      # Data Streams data stream in the same account as the subscription policy, for same-account delivery.
      # An Firehose data stream in the same account as the subscription policy, for same-account delivery. A
      # Lambda function in the same account as the subscription policy, for same-account delivery. A logical
      # destination in a different account created with PutDestination , for cross-account delivery. Kinesis
      # Data Streams and Firehose are supported as logical destinations. Each account can have one
      # account-level subscription filter policy per Region. If you are updating an existing filter, you
      # must specify the correct name in PolicyName . To perform a PutAccountPolicy subscription filter
      # operation for any destination except a Lambda function, you must also have the iam:PassRole
      # permission. Transformer policy Creates or updates a log transformer policy for your account. You use
      # log transformers to transform log events into a different format, making them easier for you to
      # process and analyze. You can also transform logs from different sources into standardized formats
      # that contain relevant, source-specific information. After you have created a transformer, CloudWatch
      # Logs performs this transformation at the time of log ingestion. You can then refer to the
      # transformed versions of the logs during operations such as querying with CloudWatch Logs Insights or
      # creating metric filters or subscription filters. You can also use a transformer to copy metadata
      # from metadata keys into the log events themselves. This metadata can include log group name, log
      # stream name, account ID and Region. A transformer for a log group is a series of processors, where
      # each processor applies one type of transformation to the log events ingested into this log group.
      # For more information about the available processors to use in a transformer, see Processors that you
      # can use . Having log events in standardized format enables visibility across your applications for
      # your log analysis, reporting, and alarming needs. CloudWatch Logs provides transformation for common
      # log types with out-of-the-box transformation templates for major Amazon Web Services log sources
      # such as VPC flow logs, Lambda, and Amazon RDS. You can use pre-built transformation templates or
      # create custom transformation policies. You can create transformers only for the log groups in the
      # Standard log class. You can have one account-level transformer policy that applies to all log groups
      # in the account. Or you can create as many as 20 account-level transformer policies that are each
      # scoped to a subset of log groups with the selectionCriteria parameter. If you have multiple
      # account-level transformer policies with selection criteria, no two of them can use the same or
      # overlapping log group name prefixes. For example, if you have one policy filtered to log groups that
      # start with my-log , you can't have another transformer policy filtered to my-logpprod or my-logging
      # . You can also set up a transformer at the log-group level. For more information, see PutTransformer
      # . If there is both a log-group level transformer created with PutTransformer and an account-level
      # transformer that could apply to the same log group, the log group uses only the log-group level
      # transformer. It ignores the account-level transformer. Field index policy You can use field index
      # policies to create indexes on fields found in log events for a log group or data source name and
      # type combination. Creating field indexes can help lower the scan volume for CloudWatch Logs Insights
      # queries that reference those fields, because these queries attempt to skip the processing of log
      # events that are known to not match the indexed field. Good fields to index are fields that you often
      # need to query for and fields or values that match only a small fraction of the total log events.
      # Common examples of indexes include request ID, session ID, user IDs, or instance IDs. For more
      # information, see Create field indexes to improve query performance and reduce costs To find the
      # fields that are in your log group events, use the GetLogGroupFields operation. To find the fields
      # for a data source use the GetLogFields operation. For example, suppose you have created a field
      # index for requestId . Then, any CloudWatch Logs Insights query on that log group that includes
      # requestId = value or requestId in [ value , value , ...] will attempt to process only the log events
      # where the indexed field matches the specified value. Matches of log events to the names of indexed
      # fields are case-sensitive. For example, an indexed field of RequestId won't match a log event
      # containing requestId . You can have one account-level field index policy that applies to all log
      # groups in the account. Or you can create as many as 20 account-level field index policies that are
      # each scoped to a subset of log groups using LogGroupNamePrefix with the selectionCriteria parameter.
      # You can have another 20 account-level field index policies using DataSourceName and DataSourceType
      # for the selectionCriteria parameter. If you have multiple account-level index policies with
      # LogGroupNamePrefix selection criteria, no two of them can use the same or overlapping log group name
      # prefixes. For example, if you have one policy filtered to log groups that start with my-log , you
      # can't have another field index policy filtered to my-logpprod or my-logging . Similarly, if you have
      # multiple account-level index policies with DataSourceName and DataSourceType selection criteria, no
      # two of them can use the same data source name and type combination. For example, if you have one
      # policy filtered to the data source name amazon_vpc and data source type flow you cannot create
      # another policy with this combination. If you create an account-level field index policy in a
      # monitoring account in cross-account observability, the policy is applied only to the monitoring
      # account and not to any source accounts. CloudWatch Logs provides default field indexes for all log
      # groups in the Standard log class. Default field indexes are automatically available for the
      # following fields: @logStream @aws.region @aws.account @source.log @data_source_name
      # @data_source_type @data_format traceId severityText attributes.session.id CloudWatch Logs provides
      # default field indexes for certain data source name and type combinations as well. Default field
      # indexes are automatically available for the following data source name and type combinations as
      # identified in the following list: amazon_vpc.flow action logStatus region flowDirection type
      # amazon_route53.resolver_query transport rcode aws_waf.access action httpRequest.country
      # aws_cloudtrail.data , aws_cloudtrail.management eventSource eventName awsRegion userAgent errorCode
      # eventType managementEvent readOnly eventCategory requestId Default field indexes are in addition to
      # any custom field indexes you define within your policy. Default field indexes are not counted
      # towards your field index quota . If you want to create a field index policy for a single log group,
      # you can use PutIndexPolicy instead of PutAccountPolicy . If you do so, that log group will use that
      # log-group level policy and any account-level policies that match at the data source level; any
      # account-level policy that matches at the log group level (for example, no selection criteria or log
      # group name prefix selection criteria) will be ignored. Metric extraction policy A metric extraction
      # policy controls whether CloudWatch Metrics can be created through the Embedded Metrics Format (EMF)
      # for log groups in your account. By default, EMF metric creation is enabled for all log groups. You
      # can use metric extraction policies to disable EMF metric creation for your entire account or
      # specific log groups. When a policy disables EMF metric creation for a log group, log events in the
      # EMF format are still ingested, but no CloudWatch Metrics are created from them. Creating a policy
      # disables metrics for AWS features that use EMF to create metrics, such as CloudWatch Container
      # Insights and CloudWatch Application Signals. To prevent turning off those features by accident, we
      # recommend that you exclude the underlying log-groups through a selection-criteria such as
      # LogGroupNamePrefix NOT IN ["/aws/containerinsights", "/aws/ecs/containerinsights",
      # "/aws/application-signals/data"] . Each account can have either one account-level metric extraction
      # policy that applies to all log groups, or up to 5 policies that are each scoped to a subset of log
      # groups with the selectionCriteria parameter. The selection criteria supports filtering by
      # LogGroupName and LogGroupNamePrefix using the operators IN and NOT IN . You can specify up to 50
      # values in each IN or NOT IN list. The selection criteria can be specified in these formats:
      # LogGroupName IN ["log-group-1", "log-group-2"] LogGroupNamePrefix NOT IN ["/aws/prefix1",
      # "/aws/prefix2"] If you have multiple account-level metric extraction policies with selection
      # criteria, no two of them can have overlapping criteria. For example, if you have one policy with
      # selection criteria LogGroupNamePrefix IN ["my-log"] , you can't have another metric extraction
      # policy with selection criteria LogGroupNamePrefix IN ["/my-log-prod"] or LogGroupNamePrefix IN
      # ["/my-logging"] , as the set of log groups matching these prefixes would be a subset of the log
      # groups matching the first policy's prefix, creating an overlap. When using NOT IN , only one policy
      # with this operator is allowed per account. When combining policies with IN and NOT IN operators, the
      # overlap check ensures that policies don't have conflicting effects. Two policies with IN and NOT IN
      # operators do not overlap if and only if every value in the IN policy is completely contained within
      # some value in the NOT IN policy. For example: If you have a NOT IN policy for prefix "/aws/lambda" ,
      # you can create an IN policy for the exact log group name "/aws/lambda/function1" because the set of
      # log groups matching "/aws/lambda/function1" is a subset of the log groups matching "/aws/lambda" .
      # If you have a NOT IN policy for prefix "/aws/lambda" , you cannot create an IN policy for prefix
      # "/aws" because the set of log groups matching "/aws" is not a subset of the log groups matching
      # "/aws/lambda" .
      def put_account_policy(
        policy_document : String,
        policy_name : String,
        policy_type : String,
        scope : String? = nil,
        selection_criteria : String? = nil
      ) : Types::PutAccountPolicyResponse
        input = Types::PutAccountPolicyRequest.new(policy_document: policy_document, policy_name: policy_name, policy_type: policy_type, scope: scope, selection_criteria: selection_criteria)
        put_account_policy(input)
      end
      def put_account_policy(input : Types::PutAccountPolicyRequest) : Types::PutAccountPolicyResponse
        request = Protocol::JsonRpc.build_request(Model::PUT_ACCOUNT_POLICY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::PutAccountPolicyResponse, "PutAccountPolicy")
      end

      # Creates a data protection policy for the specified log group. A data protection policy can help
      # safeguard sensitive data that's ingested by the log group by auditing and masking the sensitive log
      # data. Sensitive data is detected and masked when it is ingested into the log group. When you set a
      # data protection policy, log events ingested into the log group before that time are not masked. By
      # default, when a user views a log event that includes masked data, the sensitive data is replaced by
      # asterisks. A user who has the logs:Unmask permission can use a GetLogEvents or FilterLogEvents
      # operation with the unmask parameter set to true to view the unmasked log events. Users with the
      # logs:Unmask can also view unmasked data in the CloudWatch Logs console by running a CloudWatch Logs
      # Insights query with the unmask query command. For more information, including a list of types of
      # data that can be audited and masked, see Protect sensitive log data with masking . The
      # PutDataProtectionPolicy operation applies to only the specified log group. You can also use
      # PutAccountPolicy to create an account-level data protection policy that applies to all log groups in
      # the account, including both existing log groups and log groups that are created level. If a log
      # group has its own data protection policy and the account also has an account-level data protection
      # policy, then the two policies are cumulative. Any sensitive term specified in either policy is
      # masked.
      def put_data_protection_policy(
        log_group_identifier : String,
        policy_document : String
      ) : Types::PutDataProtectionPolicyResponse
        input = Types::PutDataProtectionPolicyRequest.new(log_group_identifier: log_group_identifier, policy_document: policy_document)
        put_data_protection_policy(input)
      end
      def put_data_protection_policy(input : Types::PutDataProtectionPolicyRequest) : Types::PutDataProtectionPolicyResponse
        request = Protocol::JsonRpc.build_request(Model::PUT_DATA_PROTECTION_POLICY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::PutDataProtectionPolicyResponse, "PutDataProtectionPolicy")
      end

      # Creates or updates a logical delivery destination . A delivery destination is an Amazon Web Services
      # resource that represents an Amazon Web Services service that logs can be sent to. CloudWatch Logs,
      # Amazon S3, and Firehose are supported as logs delivery destinations and X-Ray as the trace delivery
      # destination. To configure logs delivery between a supported Amazon Web Services service and a
      # destination, you must do the following: Create a delivery source, which is a logical object that
      # represents the resource that is actually sending the logs. For more information, see
      # PutDeliverySource . Use PutDeliveryDestination to create a delivery destination in the same account
      # of the actual delivery destination. The delivery destination that you create is a logical object
      # that represents the actual delivery destination. If you are delivering logs cross-account, you must
      # use PutDeliveryDestinationPolicy in the destination account to assign an IAM policy to the
      # destination. This policy allows delivery to that destination. Use CreateDelivery to create a
      # delivery by pairing exactly one delivery source and one delivery destination. For more information,
      # see CreateDelivery . You can configure a single delivery source to send logs to multiple
      # destinations by creating multiple deliveries. You can also create multiple deliveries to configure
      # multiple delivery sources to send logs to the same delivery destination. Only some Amazon Web
      # Services services support being configured as a delivery source. These services are listed as
      # Supported [V2 Permissions] in the table at Enabling logging from Amazon Web Services services. If
      # you use this operation to update an existing delivery destination, all the current delivery
      # destination parameters are overwritten with the new parameter values that you specify.
      def put_delivery_destination(
        name : String,
        delivery_destination_configuration : Types::DeliveryDestinationConfiguration? = nil,
        delivery_destination_type : String? = nil,
        output_format : String? = nil,
        tags : Hash(String, String)? = nil
      ) : Types::PutDeliveryDestinationResponse
        input = Types::PutDeliveryDestinationRequest.new(name: name, delivery_destination_configuration: delivery_destination_configuration, delivery_destination_type: delivery_destination_type, output_format: output_format, tags: tags)
        put_delivery_destination(input)
      end
      def put_delivery_destination(input : Types::PutDeliveryDestinationRequest) : Types::PutDeliveryDestinationResponse
        request = Protocol::JsonRpc.build_request(Model::PUT_DELIVERY_DESTINATION, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::PutDeliveryDestinationResponse, "PutDeliveryDestination")
      end

      # Creates and assigns an IAM policy that grants permissions to CloudWatch Logs to deliver logs
      # cross-account to a specified destination in this account. To configure the delivery of logs from an
      # Amazon Web Services service in another account to a logs delivery destination in the current
      # account, you must do the following: Create a delivery source, which is a logical object that
      # represents the resource that is actually sending the logs. For more information, see
      # PutDeliverySource . Create a delivery destination , which is a logical object that represents the
      # actual delivery destination. For more information, see PutDeliveryDestination . Use this operation
      # in the destination account to assign an IAM policy to the destination. This policy allows delivery
      # to that destination. Create a delivery by pairing exactly one delivery source and one delivery
      # destination. For more information, see CreateDelivery . Only some Amazon Web Services services
      # support being configured as a delivery source. These services are listed as Supported [V2
      # Permissions] in the table at Enabling logging from Amazon Web Services services. The contents of the
      # policy must include two statements. One statement enables general logs delivery, and the other
      # allows delivery to the chosen destination. See the examples for the needed policies.
      def put_delivery_destination_policy(
        delivery_destination_name : String,
        delivery_destination_policy : String
      ) : Types::PutDeliveryDestinationPolicyResponse
        input = Types::PutDeliveryDestinationPolicyRequest.new(delivery_destination_name: delivery_destination_name, delivery_destination_policy: delivery_destination_policy)
        put_delivery_destination_policy(input)
      end
      def put_delivery_destination_policy(input : Types::PutDeliveryDestinationPolicyRequest) : Types::PutDeliveryDestinationPolicyResponse
        request = Protocol::JsonRpc.build_request(Model::PUT_DELIVERY_DESTINATION_POLICY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::PutDeliveryDestinationPolicyResponse, "PutDeliveryDestinationPolicy")
      end

      # Creates or updates a logical delivery source . A delivery source represents an Amazon Web Services
      # resource that sends logs to an logs delivery destination. The destination can be CloudWatch Logs,
      # Amazon S3, Firehose or X-Ray for sending traces. To configure logs delivery between a delivery
      # destination and an Amazon Web Services service that is supported as a delivery source, you must do
      # the following: Use PutDeliverySource to create a delivery source, which is a logical object that
      # represents the resource that is actually sending the logs. Use PutDeliveryDestination to create a
      # delivery destination , which is a logical object that represents the actual delivery destination.
      # For more information, see PutDeliveryDestination . If you are delivering logs cross-account, you
      # must use PutDeliveryDestinationPolicy in the destination account to assign an IAM policy to the
      # destination. This policy allows delivery to that destination. Use CreateDelivery to create a
      # delivery by pairing exactly one delivery source and one delivery destination. For more information,
      # see CreateDelivery . You can configure a single delivery source to send logs to multiple
      # destinations by creating multiple deliveries. You can also create multiple deliveries to configure
      # multiple delivery sources to send logs to the same delivery destination. Only some Amazon Web
      # Services services support being configured as a delivery source. These services are listed as
      # Supported [V2 Permissions] in the table at Enabling logging from Amazon Web Services services. If
      # you use this operation to update an existing delivery source, all the current delivery source
      # parameters are overwritten with the new parameter values that you specify.
      def put_delivery_source(
        log_type : String,
        name : String,
        resource_arn : String,
        tags : Hash(String, String)? = nil
      ) : Types::PutDeliverySourceResponse
        input = Types::PutDeliverySourceRequest.new(log_type: log_type, name: name, resource_arn: resource_arn, tags: tags)
        put_delivery_source(input)
      end
      def put_delivery_source(input : Types::PutDeliverySourceRequest) : Types::PutDeliverySourceResponse
        request = Protocol::JsonRpc.build_request(Model::PUT_DELIVERY_SOURCE, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::PutDeliverySourceResponse, "PutDeliverySource")
      end

      # Creates or updates a destination. This operation is used only to create destinations for
      # cross-account subscriptions. A destination encapsulates a physical resource (such as an Amazon
      # Kinesis stream). With a destination, you can subscribe to a real-time stream of log events for a
      # different account, ingested using PutLogEvents . Through an access policy, a destination controls
      # what is written to it. By default, PutDestination does not set any access policy with the
      # destination, which means a cross-account user cannot call PutSubscriptionFilter against this
      # destination. To enable this, the destination owner must call PutDestinationPolicy after
      # PutDestination . To perform a PutDestination operation, you must also have the iam:PassRole
      # permission.
      def put_destination(
        destination_name : String,
        role_arn : String,
        target_arn : String,
        tags : Hash(String, String)? = nil
      ) : Types::PutDestinationResponse
        input = Types::PutDestinationRequest.new(destination_name: destination_name, role_arn: role_arn, target_arn: target_arn, tags: tags)
        put_destination(input)
      end
      def put_destination(input : Types::PutDestinationRequest) : Types::PutDestinationResponse
        request = Protocol::JsonRpc.build_request(Model::PUT_DESTINATION, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::PutDestinationResponse, "PutDestination")
      end

      # Creates or updates an access policy associated with an existing destination. An access policy is an
      # IAM policy document that is used to authorize claims to register a subscription filter against a
      # given destination.
      def put_destination_policy(
        access_policy : String,
        destination_name : String,
        force_update : Bool? = nil
      ) : Nil
        input = Types::PutDestinationPolicyRequest.new(access_policy: access_policy, destination_name: destination_name, force_update: force_update)
        put_destination_policy(input)
      end
      def put_destination_policy(input : Types::PutDestinationPolicyRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::PUT_DESTINATION_POLICY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Creates or updates a field index policy for the specified log group. Only log groups in the Standard
      # log class support field index policies. For more information about log classes, see Log classes .
      # You can use field index policies to create field indexes on fields found in log events in the log
      # group. Creating field indexes speeds up and lowers the costs for CloudWatch Logs Insights queries
      # that reference those field indexes, because these queries attempt to skip the processing of log
      # events that are known to not match the indexed field. Good fields to index are fields that you often
      # need to query for and fields or values that match only a small fraction of the total log events.
      # Common examples of indexes include request ID, session ID, userID, and instance IDs. For more
      # information, see Create field indexes to improve query performance and reduce costs . You can
      # configure indexed fields as facets to enable interactive exploration and filtering of your logs in
      # the CloudWatch Logs Insights console. Facets allow you to view value distributions and counts for
      # indexed fields without running queries. When you create a field index, you can optionally set it as
      # a facet to enable this interactive analysis capability. For more information, see Use facets to
      # group and explore logs . To find the fields that are in your log group events, use the
      # GetLogGroupFields operation. For example, suppose you have created a field index for requestId .
      # Then, any CloudWatch Logs Insights query on that log group that includes requestId = value or
      # requestId IN [ value , value , ...] will process fewer log events to reduce costs, and have improved
      # performance. CloudWatch Logs provides default field indexes for all log groups in the Standard log
      # class. Default field indexes are automatically available for the following fields: @logStream
      # @aws.region @aws.account @source.log traceId Default field indexes are in addition to any custom
      # field indexes you define within your policy. Default field indexes are not counted towards your
      # field index quota. Each index policy has the following quotas and restrictions: As many as 20 fields
      # can be included in the policy. Each field name can include as many as 100 characters. Matches of log
      # events to the names of indexed fields are case-sensitive. For example, a field index of RequestId
      # won't match a log event containing requestId . Log group-level field index policies created with
      # PutIndexPolicy override account-level field index policies created with PutAccountPolicy that apply
      # to log groups. If you use PutIndexPolicy to create a field index policy for a log group, that log
      # group uses only that policy for log group-level indexing, including any facet configurations. The
      # log group ignores any account-wide field index policy that applies to log groups, but data
      # source-based account policies may still apply.
      def put_index_policy(
        log_group_identifier : String,
        policy_document : String
      ) : Types::PutIndexPolicyResponse
        input = Types::PutIndexPolicyRequest.new(log_group_identifier: log_group_identifier, policy_document: policy_document)
        put_index_policy(input)
      end
      def put_index_policy(input : Types::PutIndexPolicyRequest) : Types::PutIndexPolicyResponse
        request = Protocol::JsonRpc.build_request(Model::PUT_INDEX_POLICY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::PutIndexPolicyResponse, "PutIndexPolicy")
      end

      # Creates an integration between CloudWatch Logs and another service in this account. Currently, only
      # integrations with OpenSearch Service are supported, and currently you can have only one integration
      # in your account. Integrating with OpenSearch Service makes it possible for you to create curated
      # vended logs dashboards, powered by OpenSearch Service analytics. For more information, see Vended
      # log dashboards powered by Amazon OpenSearch Service . You can use this operation only to create a
      # new integration. You can't modify an existing integration.
      def put_integration(
        integration_name : String,
        integration_type : String,
        resource_config : Types::ResourceConfig
      ) : Types::PutIntegrationResponse
        input = Types::PutIntegrationRequest.new(integration_name: integration_name, integration_type: integration_type, resource_config: resource_config)
        put_integration(input)
      end
      def put_integration(input : Types::PutIntegrationRequest) : Types::PutIntegrationResponse
        request = Protocol::JsonRpc.build_request(Model::PUT_INTEGRATION, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::PutIntegrationResponse, "PutIntegration")
      end

      # Uploads a batch of log events to the specified log stream. The sequence token is now ignored in
      # PutLogEvents actions. PutLogEvents actions are always accepted and never return
      # InvalidSequenceTokenException or DataAlreadyAcceptedException even if the sequence token is not
      # valid. You can use parallel PutLogEvents actions on the same log stream. The batch of events must
      # satisfy the following constraints: The maximum batch size is 1,048,576 bytes. This size is
      # calculated as the sum of all event messages in UTF-8, plus 26 bytes for each log event. Events more
      # than 2 hours in the future are rejected while processing remaining valid events. Events older than
      # 14 days or preceding the log group's retention period are rejected while processing remaining valid
      # events. The log events in the batch must be in chronological order by their timestamp. The timestamp
      # is the time that the event occurred, expressed as the number of milliseconds after Jan 1, 1970
      # 00:00:00 UTC . (In Amazon Web Services Tools for PowerShell and the Amazon Web Services SDK for
      # .NET, the timestamp is specified in .NET format: yyyy-mm-ddThh:mm:ss . For example,
      # 2017-09-15T13:45:30 .) A batch of log events in a single request must be in a chronological order.
      # Otherwise, the operation fails. Each log event can be no larger than 1 MB. The maximum number of log
      # events in a batch is 10,000. For valid events (within 14 days in the past to 2 hours in future), the
      # time span in a single batch cannot exceed 24 hours. Otherwise, the operation fails. The quota of
      # five requests per second per log stream has been removed. Instead, PutLogEvents actions are
      # throttled based on a per-second per-account quota. You can request an increase to the per-second
      # throttling quota by using the Service Quotas service. If a call to PutLogEvents returns
      # "UnrecognizedClientException" the most likely cause is a non-valid Amazon Web Services access key ID
      # or secret key.
      def put_log_events(
        log_events : Array(Types::InputLogEvent),
        log_group_name : String,
        log_stream_name : String,
        entity : Types::Entity? = nil,
        sequence_token : String? = nil
      ) : Types::PutLogEventsResponse
        input = Types::PutLogEventsRequest.new(log_events: log_events, log_group_name: log_group_name, log_stream_name: log_stream_name, entity: entity, sequence_token: sequence_token)
        put_log_events(input)
      end
      def put_log_events(input : Types::PutLogEventsRequest) : Types::PutLogEventsResponse
        request = Protocol::JsonRpc.build_request(Model::PUT_LOG_EVENTS, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::PutLogEventsResponse, "PutLogEvents")
      end

      # Enables or disables deletion protection for the specified log group. When enabled on a log group,
      # deletion protection blocks all deletion operations until it is explicitly disabled. For information
      # about the parameters that are common to all actions, see Common Parameters .
      def put_log_group_deletion_protection(
        deletion_protection_enabled : Bool,
        log_group_identifier : String
      ) : Nil
        input = Types::PutLogGroupDeletionProtectionRequest.new(deletion_protection_enabled: deletion_protection_enabled, log_group_identifier: log_group_identifier)
        put_log_group_deletion_protection(input)
      end
      def put_log_group_deletion_protection(input : Types::PutLogGroupDeletionProtectionRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::PUT_LOG_GROUP_DELETION_PROTECTION, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Creates or updates a metric filter and associates it with the specified log group. With metric
      # filters, you can configure rules to extract metric data from log events ingested through
      # PutLogEvents . The maximum number of metric filters that can be associated with a log group is 100.
      # Using regular expressions in filter patterns is supported. For these filters, there is a quota of
      # two regular expression patterns within a single filter pattern. There is also a quota of five
      # regular expression patterns per log group. For more information about using regular expressions in
      # filter patterns, see Filter pattern syntax for metric filters, subscription filters, filter log
      # events, and Live Tail . When you create a metric filter, you can also optionally assign a unit and
      # dimensions to the metric that is created. Metrics extracted from log events are charged as custom
      # metrics. To prevent unexpected high charges, do not specify high-cardinality fields such as
      # IPAddress or requestID as dimensions. Each different value found for a dimension is treated as a
      # separate metric and accrues charges as a separate custom metric. CloudWatch Logs might disable a
      # metric filter if it generates 1,000 different name/value pairs for your specified dimensions within
      # one hour. You can also set up a billing alarm to alert you if your charges are higher than expected.
      # For more information, see Creating a Billing Alarm to Monitor Your Estimated Amazon Web Services
      # Charges .
      def put_metric_filter(
        filter_name : String,
        filter_pattern : String,
        log_group_name : String,
        metric_transformations : Array(Types::MetricTransformation),
        apply_on_transformed_logs : Bool? = nil,
        emit_system_field_dimensions : Array(String)? = nil,
        field_selection_criteria : String? = nil
      ) : Nil
        input = Types::PutMetricFilterRequest.new(filter_name: filter_name, filter_pattern: filter_pattern, log_group_name: log_group_name, metric_transformations: metric_transformations, apply_on_transformed_logs: apply_on_transformed_logs, emit_system_field_dimensions: emit_system_field_dimensions, field_selection_criteria: field_selection_criteria)
        put_metric_filter(input)
      end
      def put_metric_filter(input : Types::PutMetricFilterRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::PUT_METRIC_FILTER, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Creates or updates a query definition for CloudWatch Logs Insights. For more information, see
      # Analyzing Log Data with CloudWatch Logs Insights . To update a query definition, specify its
      # queryDefinitionId in your request. The values of name , queryString , and logGroupNames are changed
      # to the values that you specify in your update operation. No current values are retained from the
      # current query definition. For example, imagine updating a current query definition that includes log
      # groups. If you don't specify the logGroupNames parameter in your update operation, the query
      # definition changes to contain no log groups. You must have the logs:PutQueryDefinition permission to
      # be able to perform this operation.
      def put_query_definition(
        name : String,
        query_string : String,
        client_token : String? = nil,
        log_group_names : Array(String)? = nil,
        query_definition_id : String? = nil,
        query_language : String? = nil
      ) : Types::PutQueryDefinitionResponse
        input = Types::PutQueryDefinitionRequest.new(name: name, query_string: query_string, client_token: client_token, log_group_names: log_group_names, query_definition_id: query_definition_id, query_language: query_language)
        put_query_definition(input)
      end
      def put_query_definition(input : Types::PutQueryDefinitionRequest) : Types::PutQueryDefinitionResponse
        request = Protocol::JsonRpc.build_request(Model::PUT_QUERY_DEFINITION, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::PutQueryDefinitionResponse, "PutQueryDefinition")
      end

      # Creates or updates a resource policy allowing other Amazon Web Services services to put log events
      # to this account, such as Amazon Route 53. This API has the following restrictions: Supported actions
      # - Policy only supports logs:PutLogEvents and logs:CreateLogStream actions Supported principals -
      # Policy only applies when operations are invoked by Amazon Web Services service principals (not IAM
      # users, roles, or cross-account principals Policy limits - An account can have a maximum of 10
      # policies without resourceARN and one per LogGroup resourceARN Resource policies with actions invoked
      # by non-Amazon Web Services service principals (such as IAM users, roles, or other Amazon Web
      # Services accounts) will not be enforced. For access control involving these principals, use the IAM
      # policies.
      def put_resource_policy(
        expected_revision_id : String? = nil,
        policy_document : String? = nil,
        policy_name : String? = nil,
        resource_arn : String? = nil
      ) : Types::PutResourcePolicyResponse
        input = Types::PutResourcePolicyRequest.new(expected_revision_id: expected_revision_id, policy_document: policy_document, policy_name: policy_name, resource_arn: resource_arn)
        put_resource_policy(input)
      end
      def put_resource_policy(input : Types::PutResourcePolicyRequest) : Types::PutResourcePolicyResponse
        request = Protocol::JsonRpc.build_request(Model::PUT_RESOURCE_POLICY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::PutResourcePolicyResponse, "PutResourcePolicy")
      end

      # Sets the retention of the specified log group. With a retention policy, you can configure the number
      # of days for which to retain log events in the specified log group. CloudWatch Logs doesn't
      # immediately delete log events when they reach their retention setting. It typically takes up to 72
      # hours after that before log events are deleted, but in rare situations might take longer. To
      # illustrate, imagine that you change a log group to have a longer retention setting when it contains
      # log events that are past the expiration date, but haven't been deleted. Those log events will take
      # up to 72 hours to be deleted after the new retention date is reached. To make sure that log data is
      # deleted permanently, keep a log group at its lower retention setting until 72 hours after the
      # previous retention period ends. Alternatively, wait to change the retention setting until you
      # confirm that the earlier log events are deleted. When log events reach their retention setting they
      # are marked for deletion. After they are marked for deletion, they do not add to your archival
      # storage costs anymore, even if they are not actually deleted until later. These log events marked
      # for deletion are also not included when you use an API to retrieve the storedBytes value to see how
      # many bytes a log group is storing.
      def put_retention_policy(
        log_group_name : String,
        retention_in_days : Int32
      ) : Nil
        input = Types::PutRetentionPolicyRequest.new(log_group_name: log_group_name, retention_in_days: retention_in_days)
        put_retention_policy(input)
      end
      def put_retention_policy(input : Types::PutRetentionPolicyRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::PUT_RETENTION_POLICY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Creates or updates a subscription filter and associates it with the specified log group. With
      # subscription filters, you can subscribe to a real-time stream of log events ingested through
      # PutLogEvents and have them delivered to a specific destination. When log events are sent to the
      # receiving service, they are Base64 encoded and compressed with the GZIP format. The following
      # destinations are supported for subscription filters: An Amazon Kinesis data stream belonging to the
      # same account as the subscription filter, for same-account delivery. A logical destination created
      # with PutDestination that belongs to a different account, for cross-account delivery. We currently
      # support Kinesis Data Streams and Firehose as logical destinations. An Amazon Kinesis Data Firehose
      # delivery stream that belongs to the same account as the subscription filter, for same-account
      # delivery. An Lambda function that belongs to the same account as the subscription filter, for
      # same-account delivery. Each log group can have up to two subscription filters associated with it. If
      # you are updating an existing filter, you must specify the correct name in filterName . Using regular
      # expressions in filter patterns is supported. For these filters, there is a quotas of quota of two
      # regular expression patterns within a single filter pattern. There is also a quota of five regular
      # expression patterns per log group. For more information about using regular expressions in filter
      # patterns, see Filter pattern syntax for metric filters, subscription filters, filter log events, and
      # Live Tail . To perform a PutSubscriptionFilter operation for any destination except a Lambda
      # function, you must also have the iam:PassRole permission.
      def put_subscription_filter(
        destination_arn : String,
        filter_name : String,
        filter_pattern : String,
        log_group_name : String,
        apply_on_transformed_logs : Bool? = nil,
        distribution : String? = nil,
        emit_system_fields : Array(String)? = nil,
        field_selection_criteria : String? = nil,
        role_arn : String? = nil
      ) : Nil
        input = Types::PutSubscriptionFilterRequest.new(destination_arn: destination_arn, filter_name: filter_name, filter_pattern: filter_pattern, log_group_name: log_group_name, apply_on_transformed_logs: apply_on_transformed_logs, distribution: distribution, emit_system_fields: emit_system_fields, field_selection_criteria: field_selection_criteria, role_arn: role_arn)
        put_subscription_filter(input)
      end
      def put_subscription_filter(input : Types::PutSubscriptionFilterRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::PUT_SUBSCRIPTION_FILTER, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Creates or updates a log transformer for a single log group. You use log transformers to transform
      # log events into a different format, making them easier for you to process and analyze. You can also
      # transform logs from different sources into standardized formats that contains relevant,
      # source-specific information. After you have created a transformer, CloudWatch Logs performs the
      # transformations at the time of log ingestion. You can then refer to the transformed versions of the
      # logs during operations such as querying with CloudWatch Logs Insights or creating metric filters or
      # subscription filers. You can also use a transformer to copy metadata from metadata keys into the log
      # events themselves. This metadata can include log group name, log stream name, account ID and Region.
      # A transformer for a log group is a series of processors, where each processor applies one type of
      # transformation to the log events ingested into this log group. The processors work one after
      # another, in the order that you list them, like a pipeline. For more information about the available
      # processors to use in a transformer, see Processors that you can use . Having log events in
      # standardized format enables visibility across your applications for your log analysis, reporting,
      # and alarming needs. CloudWatch Logs provides transformation for common log types with out-of-the-box
      # transformation templates for major Amazon Web Services log sources such as VPC flow logs, Lambda,
      # and Amazon RDS. You can use pre-built transformation templates or create custom transformation
      # policies. You can create transformers only for the log groups in the Standard log class. You can
      # also set up a transformer at the account level. For more information, see PutAccountPolicy . If
      # there is both a log-group level transformer created with PutTransformer and an account-level
      # transformer that could apply to the same log group, the log group uses only the log-group level
      # transformer. It ignores the account-level transformer.
      def put_transformer(
        log_group_identifier : String,
        transformer_config : Array(Types::Processor)
      ) : Nil
        input = Types::PutTransformerRequest.new(log_group_identifier: log_group_identifier, transformer_config: transformer_config)
        put_transformer(input)
      end
      def put_transformer(input : Types::PutTransformerRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::PUT_TRANSFORMER, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Starts a Live Tail streaming session for one or more log groups. A Live Tail session returns a
      # stream of log events that have been recently ingested in the log groups. For more information, see
      # Use Live Tail to view logs in near real time . The response to this operation is a response stream,
      # over which the server sends live log events and the client receives them. The following objects are
      # sent over the stream: A single LiveTailSessionStart object is sent at the start of the session.
      # Every second, a LiveTailSessionUpdate object is sent. Each of these objects contains an array of the
      # actual log events. If no new log events were ingested in the past second, the LiveTailSessionUpdate
      # object will contain an empty array. The array of log events contained in a LiveTailSessionUpdate can
      # include as many as 500 log events. If the number of log events matching the request exceeds 500 per
      # second, the log events are sampled down to 500 log events to be included in each
      # LiveTailSessionUpdate object. If your client consumes the log events slower than the server produces
      # them, CloudWatch Logs buffers up to 10 LiveTailSessionUpdate events or 5000 log events, after which
      # it starts dropping the oldest events. A SessionStreamingException object is returned if an unknown
      # error occurs on the server side. A SessionTimeoutException object is returned when the session times
      # out, after it has been kept open for three hours. The StartLiveTail API routes requests to
      # streaming-logs. Region .amazonaws.com using SDK host prefix injection. VPC endpoint support is not
      # available for this API. You can end a session before it times out by closing the session stream or
      # by closing the client that is receiving the stream. The session also ends if the established
      # connection between the client and the server breaks. For examples of using an SDK to start a Live
      # Tail session, see Start a Live Tail session using an Amazon Web Services SDK .
      def start_live_tail(
        log_group_identifiers : Array(String),
        log_event_filter_pattern : String? = nil,
        log_stream_name_prefixes : Array(String)? = nil,
        log_stream_names : Array(String)? = nil
      ) : Types::StartLiveTailResponse
        input = Types::StartLiveTailRequest.new(log_group_identifiers: log_group_identifiers, log_event_filter_pattern: log_event_filter_pattern, log_stream_name_prefixes: log_stream_name_prefixes, log_stream_names: log_stream_names)
        start_live_tail(input)
      end
      def start_live_tail(input : Types::StartLiveTailRequest) : Types::StartLiveTailResponse
        request = Protocol::JsonRpc.build_request(Model::START_LIVE_TAIL, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::StartLiveTailResponse, "StartLiveTail")
      end

      # Starts a query of one or more log groups or data sources using CloudWatch Logs Insights. You specify
      # the log groups or data sources and time range to query and the query string to use. You can query up
      # to 10 data sources in a single query. For more information, see CloudWatch Logs Insights Query
      # Syntax . After you run a query using StartQuery , the query results are stored by CloudWatch Logs.
      # You can use GetQueryResults to retrieve the results of a query, using the queryId that StartQuery
      # returns. Interactive queries started with StartQuery share concurrency limits with automated
      # scheduled query executions. Both types of queries count toward the same regional concurrent query
      # quota, so high scheduled query activity may affect the availability of concurrent slots for
      # interactive queries. To specify the log groups to query, a StartQuery operation must include one of
      # the following: Either exactly one of the following parameters: logGroupName , logGroupNames , or
      # logGroupIdentifiers Or the queryString must include a SOURCE command to select log groups for the
      # query. The SOURCE command can select log groups based on log group name prefix, account ID, and log
      # class, or select data sources using dataSource syntax in LogsQL, PPL, and SQL. For more information
      # about the SOURCE command, see SOURCE . If you have associated a KMS key with the query results in
      # this account, then StartQuery uses that key to encrypt the results when it stores them. If no key is
      # associated with query results, the query results are encrypted with the default CloudWatch Logs
      # encryption method. Queries time out after 60 minutes of runtime. If your queries are timing out,
      # reduce the time range being searched or partition your query into a number of queries. If you are
      # using CloudWatch cross-account observability, you can use this operation in a monitoring account to
      # start a query in a linked source account. For more information, see CloudWatch cross-account
      # observability . For a cross-account StartQuery operation, the query definition must be defined in
      # the monitoring account. You can have up to 30 concurrent CloudWatch Logs insights queries, including
      # queries that have been added to dashboards.
      def start_query(
        end_time : Int64,
        query_string : String,
        start_time : Int64,
        limit : Int32? = nil,
        log_group_identifiers : Array(String)? = nil,
        log_group_name : String? = nil,
        log_group_names : Array(String)? = nil,
        query_language : String? = nil
      ) : Types::StartQueryResponse
        input = Types::StartQueryRequest.new(end_time: end_time, query_string: query_string, start_time: start_time, limit: limit, log_group_identifiers: log_group_identifiers, log_group_name: log_group_name, log_group_names: log_group_names, query_language: query_language)
        start_query(input)
      end
      def start_query(input : Types::StartQueryRequest) : Types::StartQueryResponse
        request = Protocol::JsonRpc.build_request(Model::START_QUERY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::StartQueryResponse, "StartQuery")
      end

      # Stops a CloudWatch Logs Insights query that is in progress. If the query has already ended, the
      # operation returns an error indicating that the specified query is not running. This operation can be
      # used to cancel both interactive queries and individual scheduled query executions. When used with
      # scheduled queries, StopQuery cancels only the specific execution identified by the query ID, not the
      # scheduled query configuration itself.
      def stop_query(
        query_id : String
      ) : Types::StopQueryResponse
        input = Types::StopQueryRequest.new(query_id: query_id)
        stop_query(input)
      end
      def stop_query(input : Types::StopQueryRequest) : Types::StopQueryResponse
        request = Protocol::JsonRpc.build_request(Model::STOP_QUERY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::StopQueryResponse, "StopQuery")
      end

      # The TagLogGroup operation is on the path to deprecation. We recommend that you use TagResource
      # instead. Adds or updates the specified tags for the specified log group. To list the tags for a log
      # group, use ListTagsForResource . To remove tags, use UntagResource . For more information about
      # tags, see Tag Log Groups in Amazon CloudWatch Logs in the Amazon CloudWatch Logs User Guide .
      # CloudWatch Logs doesn't support IAM policies that prevent users from assigning specified tags to log
      # groups using the aws:Resource/ key-name or aws:TagKeys condition keys. For more information about
      # using tags to control access, see Controlling access to Amazon Web Services resources using tags .
      def tag_log_group(
        log_group_name : String,
        tags : Hash(String, String)
      ) : Nil
        input = Types::TagLogGroupRequest.new(log_group_name: log_group_name, tags: tags)
        tag_log_group(input)
      end
      def tag_log_group(input : Types::TagLogGroupRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::TAG_LOG_GROUP, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Assigns one or more tags (key-value pairs) to the specified CloudWatch Logs resource. Currently, the
      # only CloudWatch Logs resources that can be tagged are log groups and destinations. Tags can help you
      # organize and categorize your resources. You can also use them to scope user permissions by granting
      # a user permission to access or change only resources with certain tag values. Tags don't have any
      # semantic meaning to Amazon Web Services and are interpreted strictly as strings of characters. You
      # can use the TagResource action with a resource that already has tags. If you specify a new tag key
      # for the alarm, this tag is appended to the list of tags associated with the alarm. If you specify a
      # tag key that is already associated with the alarm, the new tag value that you specify replaces the
      # previous value for that tag. You can associate as many as 50 tags with a CloudWatch Logs resource.
      def tag_resource(
        resource_arn : String,
        tags : Hash(String, String)
      ) : Nil
        input = Types::TagResourceRequest.new(resource_arn: resource_arn, tags: tags)
        tag_resource(input)
      end
      def tag_resource(input : Types::TagResourceRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::TAG_RESOURCE, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Tests the filter pattern of a metric filter against a sample of log event messages. You can use this
      # operation to validate the correctness of a metric filter pattern.
      def test_metric_filter(
        filter_pattern : String,
        log_event_messages : Array(String)
      ) : Types::TestMetricFilterResponse
        input = Types::TestMetricFilterRequest.new(filter_pattern: filter_pattern, log_event_messages: log_event_messages)
        test_metric_filter(input)
      end
      def test_metric_filter(input : Types::TestMetricFilterRequest) : Types::TestMetricFilterResponse
        request = Protocol::JsonRpc.build_request(Model::TEST_METRIC_FILTER, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::TestMetricFilterResponse, "TestMetricFilter")
      end

      # Use this operation to test a log transformer. You enter the transformer configuration and a set of
      # log events to test with. The operation responds with an array that includes the original log events
      # and the transformed versions.
      def test_transformer(
        log_event_messages : Array(String),
        transformer_config : Array(Types::Processor)
      ) : Types::TestTransformerResponse
        input = Types::TestTransformerRequest.new(log_event_messages: log_event_messages, transformer_config: transformer_config)
        test_transformer(input)
      end
      def test_transformer(input : Types::TestTransformerRequest) : Types::TestTransformerResponse
        request = Protocol::JsonRpc.build_request(Model::TEST_TRANSFORMER, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::TestTransformerResponse, "TestTransformer")
      end

      # The UntagLogGroup operation is on the path to deprecation. We recommend that you use UntagResource
      # instead. Removes the specified tags from the specified log group. To list the tags for a log group,
      # use ListTagsForResource . To add tags, use TagResource . When using IAM policies to control tag
      # management for CloudWatch Logs log groups, the condition keys aws:Resource/key-name and aws:TagKeys
      # cannot be used to restrict which tags users can assign.
      def untag_log_group(
        log_group_name : String,
        tags : Array(String)
      ) : Nil
        input = Types::UntagLogGroupRequest.new(log_group_name: log_group_name, tags: tags)
        untag_log_group(input)
      end
      def untag_log_group(input : Types::UntagLogGroupRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::UNTAG_LOG_GROUP, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Removes one or more tags from the specified resource.
      def untag_resource(
        resource_arn : String,
        tag_keys : Array(String)
      ) : Nil
        input = Types::UntagResourceRequest.new(resource_arn: resource_arn, tag_keys: tag_keys)
        untag_resource(input)
      end
      def untag_resource(input : Types::UntagResourceRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::UNTAG_RESOURCE, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Use this operation to suppress anomaly detection for a specified anomaly or pattern. If you suppress
      # an anomaly, CloudWatch Logs won't report new occurrences of that anomaly and won't update that
      # anomaly with new data. If you suppress a pattern, CloudWatch Logs won't report any anomalies related
      # to that pattern. You must specify either anomalyId or patternId , but you can't specify both
      # parameters in the same operation. If you have previously used this operation to suppress detection
      # of a pattern or anomaly, you can use it again to cause CloudWatch Logs to end the suppression. To do
      # this, use this operation and specify the anomaly or pattern to stop suppressing, and omit the
      # suppressionType and suppressionPeriod parameters.
      def update_anomaly(
        anomaly_detector_arn : String,
        anomaly_id : String? = nil,
        baseline : Bool? = nil,
        pattern_id : String? = nil,
        suppression_period : Types::SuppressionPeriod? = nil,
        suppression_type : String? = nil
      ) : Nil
        input = Types::UpdateAnomalyRequest.new(anomaly_detector_arn: anomaly_detector_arn, anomaly_id: anomaly_id, baseline: baseline, pattern_id: pattern_id, suppression_period: suppression_period, suppression_type: suppression_type)
        update_anomaly(input)
      end
      def update_anomaly(input : Types::UpdateAnomalyRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::UPDATE_ANOMALY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Use this operation to update the configuration of a delivery to change either the S3 path pattern or
      # the format of the delivered logs. You can't use this operation to change the source or destination
      # of the delivery.
      def update_delivery_configuration(
        id : String,
        field_delimiter : String? = nil,
        record_fields : Array(String)? = nil,
        s3_delivery_configuration : Types::S3DeliveryConfiguration? = nil
      ) : Types::UpdateDeliveryConfigurationResponse
        input = Types::UpdateDeliveryConfigurationRequest.new(id: id, field_delimiter: field_delimiter, record_fields: record_fields, s3_delivery_configuration: s3_delivery_configuration)
        update_delivery_configuration(input)
      end
      def update_delivery_configuration(input : Types::UpdateDeliveryConfigurationRequest) : Types::UpdateDeliveryConfigurationResponse
        request = Protocol::JsonRpc.build_request(Model::UPDATE_DELIVERY_CONFIGURATION, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::UpdateDeliveryConfigurationResponse, "UpdateDeliveryConfiguration")
      end

      # Updates an existing log anomaly detector.
      def update_log_anomaly_detector(
        anomaly_detector_arn : String,
        enabled : Bool,
        anomaly_visibility_time : Int64? = nil,
        evaluation_frequency : String? = nil,
        filter_pattern : String? = nil
      ) : Nil
        input = Types::UpdateLogAnomalyDetectorRequest.new(anomaly_detector_arn: anomaly_detector_arn, enabled: enabled, anomaly_visibility_time: anomaly_visibility_time, evaluation_frequency: evaluation_frequency, filter_pattern: filter_pattern)
        update_log_anomaly_detector(input)
      end
      def update_log_anomaly_detector(input : Types::UpdateLogAnomalyDetectorRequest) : Nil
        request = Protocol::JsonRpc.build_request(Model::UPDATE_LOG_ANOMALY_DETECTOR, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        nil
      end

      # Updates an existing scheduled query with new configuration. This operation uses PUT semantics,
      # allowing modification of query parameters, schedule, and destinations.
      def update_scheduled_query(
        execution_role_arn : String,
        identifier : String,
        query_language : String,
        query_string : String,
        schedule_expression : String,
        description : String? = nil,
        destination_configuration : Types::DestinationConfiguration? = nil,
        log_group_identifiers : Array(String)? = nil,
        schedule_end_time : Int64? = nil,
        schedule_start_time : Int64? = nil,
        start_time_offset : Int64? = nil,
        state : String? = nil,
        timezone : String? = nil
      ) : Types::UpdateScheduledQueryResponse
        input = Types::UpdateScheduledQueryRequest.new(execution_role_arn: execution_role_arn, identifier: identifier, query_language: query_language, query_string: query_string, schedule_expression: schedule_expression, description: description, destination_configuration: destination_configuration, log_group_identifiers: log_group_identifiers, schedule_end_time: schedule_end_time, schedule_start_time: schedule_start_time, start_time_offset: start_time_offset, state: state, timezone: timezone)
        update_scheduled_query(input)
      end
      def update_scheduled_query(input : Types::UpdateScheduledQueryRequest) : Types::UpdateScheduledQueryResponse
        request = Protocol::JsonRpc.build_request(Model::UPDATE_SCHEDULED_QUERY, input, endpoint)
        request = request.with_headers(endpoint_headers)
        response = runtime.execute(request)
        raise Protocol::JsonRpc.parse_error(response) if response.status >= 300
        Protocol::JsonRpc.parse_response(response, Types::UpdateScheduledQueryResponse, "UpdateScheduledQuery")
      end
    end
  end
end
